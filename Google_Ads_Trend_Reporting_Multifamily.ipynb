{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdmarvin1/glitter-ads/blob/main/Google_Ads_Trend_Reporting_Multifamily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3xwZlUMCKxo9",
        "outputId": "9b2a09e9-a3ef-4985-c4d9-29e551c49124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-ads in /usr/local/lib/python3.12/dist-packages (28.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (2.181.0)\n",
            "Requirement already satisfied: google-api-core<=3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from google-ads) (2.25.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.3 in /usr/local/lib/python3.12/dist-packages (from google-ads) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.59.0 in /usr/local/lib/python3.12/dist-packages (from google-ads) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.59.0 in /usr/local/lib/python3.12/dist-packages (from google-ads) (1.71.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ads) (1.26.1)\n",
            "Requirement already satisfied: PyYAML<7.0,>=5.1 in /usr/local/lib/python3.12/dist-packages (from google-ads) (6.0.2)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from google-ads) (5.29.5)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib) (2.38.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.30.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core<=3.0.0,>=2.13.0->google-ads) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (4.9.1)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-auth-oauthlib) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<=3.0.0,>=2.13.0->google-ads) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<=3.0.0,>=2.13.0->google-ads) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<=3.0.0,>=2.13.0->google-ads) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<=3.0.0,>=2.13.0->google-ads) (2025.8.3)\n",
            "--- Initializing Google Ads Client ---\n",
            "✅ Google Ads client initialized successfully!\n",
            "--- Initializing Google Docs Client ---\n",
            "✅ Google Docs client initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: INSTALL REQUIRED LIBRARIES\n",
        "# ==============================================================================\n",
        "!pip install google-ads google-auth-oauthlib gspread python-dateutil google-api-python-client\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: IMPORTS & AUTHENTICATION\n",
        "# ==============================================================================\n",
        "import gspread\n",
        "from gspread.utils import rowcol_to_a1\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "from google.ads.googleads.client import GoogleAdsClient\n",
        "from google.ads.googleads.errors import GoogleAdsException\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "# --- NEW: Import for Google Docs API ---\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "\n",
        "# --- UPDATED: Authenticate with all required scopes (Sheets, Drive, Docs) ---\n",
        "auth.authenticate_user()\n",
        "SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/documents']\n",
        "creds, _ = default(scopes=SCOPES)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "# --- Google Ads API Configuration ---\n",
        "google_ads_client = None\n",
        "try:\n",
        "    print(\"--- Initializing Google Ads Client ---\")\n",
        "    credentials = {\n",
        "        \"developer_token\": userdata.get('ADS_DEVELOPER_TOKEN'),\n",
        "        \"client_id\": userdata.get('ADS_CLIENT_ID'),\n",
        "        \"client_secret\": userdata.get('ADS_CLIENT_SECRET'),\n",
        "        \"refresh_token\": userdata.get('ADS_REFRESH_TOKEN'),\n",
        "        \"login_customer_id\": userdata.get('ADS_LOGIN_CUSTOMER_ID'),\n",
        "        \"use_proto_plus\": True\n",
        "    }\n",
        "    google_ads_client = GoogleAdsClient.load_from_dict(credentials)\n",
        "    print(\"✅ Google Ads client initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An unexpected error occurred during setup: {e}\")\n",
        "\n",
        "\n",
        "# --- NEW: Google Docs API Configuration ---\n",
        "DOCS_LOG_ID = '1ZMv52EXlCDNX0sWBDcQRNdvuOxRcdOogMy0jHGvVgTE'\n",
        "docs_service = None\n",
        "try:\n",
        "    print(\"--- Initializing Google Docs Client ---\")\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "    print(\"✅ Google Docs client initialized successfully!\")\n",
        "except HttpError as e:\n",
        "    print(f\"\\n❌ An error occurred during Google Docs client setup: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An unexpected error occurred during setup: {e}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: CONFIGURATION\n",
        "# ==============================================================================\n",
        "KEYWORD_GROUPS = {\n",
        "    'Bedrooms': ['1 bed 1 bath apartments', '1 bed 1 bath for rent', '1 bed apartment for rent', '1 bed apartments near me', '1 bedroom and den apartments near me', '1 bedroom apartments', '1 bedroom apartments for rent', '1 bedroom apartments for rent near me', '1 bedroom apartments near me', '1 bedroom condo', '1 bedroom condo for rent', '1 bedroom condo for rent near me', '1 bedroom efficiency apartment for rent', '1 bedroom efficiency apartments near me', '1 bedroom for rent', '1 bedroom for rent near me', '1 bedroom studio for rent', '1 bedroom townhomes for rent', '1 bedroom townhouse', '1 br apartments near me', '1 room apartment for rent', '1bed 1bath for rent', '1bed 1bath for rent near me', '2 bdrm apt for rent', '2 bed 1 bath apartments', '2 bed 2 bath', '2 bed 2 bath apartments', '2 bed 2 bath apartments near me', '2 bed 2 bath for rent', '2 bed 2 bath townhomes for rent', '2 bed apartments', '2 bed apartments for rent', '2 bed apartments near me', '2 bed townhouse for rent', '2 bedroom 2 bath apartments', '2 bedroom apartments', '2 bedroom apartments for rent', '2 bedroom apartments for rent near me', '2 bedroom apartments near me', '2 bedroom condo for rent', '2 bedroom condo for rent near me', '2 bedroom for rent', '2 bedroom for rent near me', '2 bedroom property to rent', '2bedroom 2bath for rent', '3 bed apartments', '3 bed apartments near me', '3 bed townhouse for rent', '3 bedroom apartment complexes near me', '3 bedroom apartments', '3 bedroom apartments for rent', '3 bedroom apartments for rent near me', '3 bedroom apartments near me', '3 bedroom condo for rent', '3 bedroom condos for rent near me', '3 bedroom for rent', '3 bedroom for rent near me', '3 bedroom townhomes for rent', '3 bedroom townhomes for rent near me', '3 bedroom townhouses for rent', '3bedroom 2bath for rent', '4 bed 2 bath apartments', '4 bed townhouse for rent', '4 bedroom 2 bath apartments', '4 bedroom 4 bathroom apartment', '4 bedroom apartments', '4 bedroom apartments for rent', '4 bedroom apartments for rent near me', '4 bedroom apartments near me', '4 bedroom condo for rent', '4 bedroom townhomes for rent', '4 bedroom townhomes for rent near me', '4 bedroom townhouses for rent', '4 bedroom townhouses for rent near me', 'four bedroom apartments', 'modern 1 bedroom apartments', 'modern 2 bedroom apartment', 'modern one bedroom apartment', 'one bed apartment for rent', 'one bedroom apartment for rent', 'one bedroom apartment for rent near me', 'one bedroom apartments', 'one bedroom apartments near me', 'one bedroom condo', 'one bedroom condo for rent', 'one bedroom townhomes for rent', 'one room apartment for rent', 'three bedroom apartments for rent', 'three bedroom apartments near me', 'three bedroom condos for rent', 'three bedroom townhomes for rent', 'two bedroom apartments', 'two bedroom apartments near me', 'two bedroom condo for rent', 'two bedroom townhomes for rent', 'two bedroom townhouses for rent'],\n",
        "    'Property Type': ['apartment townhomes for rent', 'apartment townhomes for rent near me', 'apartments', 'apartments and condos for rent', 'apartments and condos for rent near me', 'apartments and townhomes for rent', 'apartments and townhomes for rent near me', 'apartments and townhomes near me', 'bachelor apartment', 'bedroom studio', 'chicago industrial loft', 'chicago industrial lofts for rent', 'condo apartments for rent', 'condo townhomes for rent', 'condominium for rent', 'condominiums for rent near me', 'condos and townhomes for rent', 'condos and townhomes for rent near me', 'condos for lease', 'condos for lease near me', 'condos for rent', 'condos for rent by owner', 'condos for rent by owner near me', 'condos for rent near me', 'downtown apartments for rent', 'downtown condos for rent', 'duplex apartment for rent', 'duplex condo for rent', 'duplex for rent', 'duplex for rent near me', 'efficiencies for rent', 'efficiencies near me', 'efficiency apartment', 'efficiency apartment for rent', 'efficiency apartment near me', 'efficiency apartments near me', 'efficiency for rent', 'efficiency for rent near me', 'high rise apartments', 'high rise apartments for rent', 'high rise apartments near me', 'high rise condos for rent', 'houses and townhomes for rent', 'industrial apartments', 'industrial apartments for rent', 'industrial apartments near me', 'industrial loft apartments', 'industrial lofts', 'industrial lofts for rent', 'industrial studio apartment', 'industrial style apartments', 'industrial style lofts', 'industrial warehouse apartment', 'loft apartment', 'loft apartments for rent', 'loft apartments for rent near me', 'loft apartments near me', 'loft for rent', 'loft for rent near me', 'loft style apartments', 'loft style apartments near me', 'lofts near me', 'places to rent near me', 'private apartments for rent', 'private condos for rent', 'private owned apartments for rent', 'private owned townhomes for rent', 'private owned townhomes for rent near me', 'privately owned condos for rent', 'rentals near me', 'service apartment', 'service apartments near me', 'serviced apartments', 'single apartments near me', 'small apartments for rent', 'small studio apartment', 'studio apartment complexes near me', 'studio apartment for sale', 'studio apartments', 'studio apartments for rent', 'studio apartments for rent near me', 'studio apartments near me', 'studio apts for rent near me', 'studio apts near me', 'studio condo for rent', 'studio loft apartment', 'studios for rent', 'studios for rent near me', 'tiny apartment', 'town home for rent near me', 'town house rentals near me', 'townhome apartments', 'townhome apartments near me', 'townhome style apartments', 'townhomes and condos for rent by owner', 'townhomes and duplexes for rent', 'townhomes for lease near me', 'townhomes for rent', 'townhomes for rent by owner', 'townhomes for rent by private owner', 'townhomes for rent near me', 'townhomes near me', 'townhomes near me for rent by owner', 'townhouse apartments', 'townhouse apartments for rent', 'townhouse apartments near me', 'townhouse for lease near me', 'townhouses for rent', 'townhouses for rent near me', 'warehouse industrial loft apartment'],\n",
        "    'Features': ['2 car garage with apartment', '2 car garage with loft apartment', '2 story garage with apartment', '24x24 garage with apartment cost', '4 car garage with apartment', 'above garage apartment for rent near me', 'all utilities included apartments near me', 'apartment complex with garage near me', 'apartment complexes near me with pools', 'apartment complexes with pools near me', 'apartments for rent by owner near me', 'apartments for rent near me pet friendly', 'apartments for rent pets allowed', 'apartments for rent that allow dogs', 'apartments for rent with utilities included', 'apartments near me with attached garages', 'apartments no breed restrictions', 'apartments that allow 3 pets near me', 'apartments that allow big dogs', 'apartments that allow dogs', 'apartments that allow dogs near me', 'apartments that allow large dogs', 'apartments that allow pets', 'apartments that allow pets near me', 'apartments that allow pit bulls', 'apartments that allow pit bulls near me', 'apartments with all utilities included', 'apartments with attached garages', 'apartments with attached garages for rent', 'apartments with attached garages near me', 'apartments with dog parks', 'apartments with garages', 'apartments with garages near me', 'apartments with utilities included', 'apartments with washer and dryer in unit near me', 'brand new apartment complexes near me', 'brand new apartments near me', 'cat friendly apartments near me', 'condos with garages for rent', 'condos with pools near me', 'detached garage with apartment', 'dog friendly apartment complexes near me', 'dog friendly apartments', 'dog friendly apartments for rent', 'dog friendly apartments for rent near me', 'dog friendly apartments near me', 'garage with apartment above', 'garage with loft apartment', 'gated apartment complex near me', 'gated apartments near me', 'large dog friendly apartments', 'metal garage with apartment', 'modern apartments near me', 'modern industrial apartments', 'modular garage with apartment', 'new apartment being built near me', 'new apartment buildings near me', 'new apartment complexes near me', 'new apartments for rent', 'new apartments for rent near me', 'new apartments for sale near me', 'new apartments near me', 'new apartments near me for rent', 'new construction apartments near me', 'new renovated apartments near me', 'new townhomes for rent', 'new townhomes for rent near me', 'newest apartments near me', 'newly built apartments near me', 'pet friendly apartment complexes near me', 'pet friendly apartments', 'pet friendly apartments for rent', 'pet friendly apartments near me', 'pet friendly apts near me', 'pet friendly condos for rent', 'pet friendly places for rent', 'pet friendly places for rent near me', 'pet friendly townhomes for rent', 'pet friendly townhomes for rent near me', 'places for rent that allow dogs', 'rentals near me pet friendly', 'student apartments near me', 'student housing near me', 'student living apartments near me', 'townhomes for rent near me pet friendly', 'townhomes with basement for rent', 'townhomes with garages for rent'],\n",
        "    'Price': ['2nd chance apartments near me', 'affordable 2 bedroom apartments', 'affordable apartment complexes near me', 'affordable apartments', 'affordable apartments for rent', 'affordable apartments for rent near me', 'affordable apartments near me', 'affordable luxury apartments', 'affordable luxury apartments near me', 'affordable rental housing', 'affordable rentals near me', 'affordable studio apartments', 'affordable studio apartments near me', 'affordable townhomes for rent', 'affordable townhomes for rent near me', 'affordable townhomes near me', 'apartments for rent with move in specials', 'apartments with move in specials', 'apartments with move in specials near me', 'cheap 1 bedroom apartments', 'cheap 1 bedroom apartments for rent', 'cheap 1 bedroom apartments near me', 'cheap 2 bedroom apartments', 'cheap 2 bedroom apartments for rent near me', 'cheap 2 bedroom apartments near me', 'cheap 3 bedroom apartments', 'cheap 3 bedroom apartments near me', 'cheap 4 bedroom apartments', 'cheap affordable apartments', 'cheap apartment complexes near me', 'cheap apartments', 'cheap apartments for rent', 'cheap apartments for rent near me', 'cheap apartments for rent with utilities included near me', 'cheap apartments near me', 'cheap apts near me', 'cheap condos for rent', 'cheap condos for rent near me', 'cheap condos near me', 'cheap efficiency for rent near me', 'cheap lofts for rent near me', 'cheap luxury apartments', 'cheap luxury apartments near me', 'cheap one bedroom apartments', 'cheap pet friendly apartments', 'cheap pet friendly apartments near me', 'cheap rentals near me', 'cheap studio apartments', 'cheap studio apartments for rent', 'cheap studio apartments near me', 'cheap studios for rent', 'cheap studios for rent near me', 'cheap studios near me', 'cheap townhomes', 'cheap townhomes for rent', 'cheap townhomes for rent near me', 'cheap townhomes near me', 'efficiency for rent under $500', 'immediate move in specials near me', 'low income pet friendly apartments near me', 'low income studio apartments', 'low rent apartments near me', 'luxury apartment complex near me', 'luxury apartments', 'luxury apartments for rent', 'luxury apartments for rent near me', 'luxury apartments near me', 'luxury apts near me', 'luxury condos for rent', 'luxury condos near me', 'luxury high rise apartments', 'luxury loft apartments', 'luxury rentals near me', 'luxury studio apartments', 'luxury townhomes for rent', 'luxury townhomes for rent near me', 'luxury townhomes near me', 'modern luxury apartments', 'move in specials', 'move in specials near me', 'rent specials near me', 'rent to own condo', 'rent to own condos near me', 'rent to own townhomes near me', 'second chance apartments near me', 'second chance apartments with move in specials near me', 'second chance rentals near me', 'section 8 apartments', 'section 8 apartments near me', 'section 8 rentals'],\n",
        "    'Lease Type': ['3 month lease apartments', '6 month lease apartments', 'apartments available now', 'apartments available now near me', 'apartments for lease near me', 'apartments for rent', 'apartments for rent month to month', 'apartments for rent near me', 'apartments month to month', 'apartments near me', 'apartments that lease to corporations', 'apt complex near me', 'apt for rent', 'available apartments near me', 'best apartment complexes near me', 'best apartments near me', 'corporate apartment rentals', 'corporate apartments', 'corporate apartments near me', 'corporate furnished apartments', 'corporate housing', 'corporate housing near me', 'corporate lease apartments', 'corporate lease apartments near me', 'corporate rentals', 'corporate short term rentals', 'extended stay apartments', 'extended stay apartments near me', 'extended stay rental', 'extended stay rentals near me', 'fully furnished apartment for rent', 'fully furnished apartments', 'fully furnished apartments for rent near me', 'fully furnished apartments near me', 'fully furnished condo for rent', 'fully furnished rentals', 'fully furnished rentals near me', 'furnished 1 bedroom apartment', 'furnished 1 bedroom apartment for rent', 'furnished apartments', 'furnished apartments for rent', 'furnished apartments for rent near me', 'furnished apartments for rent short term', 'furnished apartments near me', 'furnished apts for rent', 'furnished apts for rent near me', 'furnished condos for rent', 'furnished condos for rent near me', 'furnished corporate rentals near me', 'furnished efficiency apartments near me', 'furnished month to month rentals', 'furnished one bedroom apartments', 'furnished one bedroom apartments near me', 'furnished rentals near me', 'furnished short term rentals', 'furnished short term rentals near me', 'furnished studio apartments', 'furnished studio apartments for rent', 'furnished studio apartments near me', 'furnished studio for rent', 'furnished temporary housing', 'furnished townhomes for rent', 'good apartments near me', 'immediate move in apartments near me', 'local apartments for rent', 'long term corporate housing', 'month to month apartment lease', 'month to month apartment rentals', 'month to month apartment rentals near me', 'month to month apartments', 'month to month apartments near me', 'month to month furnished apartments', 'month to month furnished rentals', 'month to month lease apartments near me', 'month to month lease near me', 'month to month rent near me', 'month to month rentals', 'monthly furnished rentals', 'monthly rentals near me', 'nice apartments', 'nice apartments near me', 'nice townhomes for rent', 'rental month to month', 'short lease apartments near me', 'short stay apartments', 'short term apartment rentals', 'short term apartment rentals near me', 'short term apartments near me', 'short term condo rentals', 'short term corporate housing', 'short term fully furnished rentals', 'short term furnished apartments', 'short term house rentals', 'short term house rentals near me', 'short term housing', 'short term housing near me', 'short term lease', 'short term lease apartments', 'short term lease apartments near me', 'short term lease near me', 'short term monthly rentals', 'short term pet friendly rentals near me', 'short term property rental', 'short term rentals', 'short term rentals near me', 'temporary corporate housing', 'temporary furnished apartments']\n",
        "}\n",
        "SPREADSHEET_URL = 'https://docs.google.com/spreadsheets/d/1pI2NEKXAjJhaqcUy-w4-4Wyas75B-6cg3MZMKVEBYkg/edit?gid=0#gid=0'\n",
        "US_STATES = [\n",
        "  \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "US_CITIES = [\n",
        "  \"New York, NY\", \"Los Angeles, CA\", \"Chicago, IL\", \"Houston, TX\", \"Phoenix, AZ\", \"Philadelphia, PA\", \"San Antonio, TX\", \"San Diego, CA\", \"Dallas, TX\", \"Jacksonville, FL\", \"Fort Worth, TX\", \"San Jose, CA\", \"Austin, TX\", \"Charlotte, NC\", \"Columbus, OH\", \"Indianapolis, IN\", \"San Francisco, CA\", \"Seattle, WA\", \"Denver, CO\", \"Oklahoma City, OK\", \"Nashville, TN\", \"Washington, DC\", \"El Paso, TX\", \"Las Vegas, NV\", \"Boston, MA\", \"Detroit, MI\", \"Louisville, KY\", \"Portland, OR\", \"Memphis, TN\", \"Baltimore, MD\", \"Milwaukee, WI\", \"Albuquerque, NM\", \"Tucson, AZ\", \"Fresno, CA\", \"Sacramento, CA\", \"Atlanta, GA\", \"Mesa, AZ\", \"Kansas City, MO\", \"Raleigh, NC\", \"Colorado Springs, CO\", \"Omaha, NE\", \"Miami, FL\", \"Virginia Beach, VA\", \"Long Beach, CA\", \"Oakland, CA\", \"Minneapolis, MN\", \"Bakersfield, CA\", \"Tulsa, OK\", \"Tampa, FL\", \"Arlington, TX\", \"Aurora, CO\", \"Wichita, KS\", \"Cleveland, OH\", \"New Orleans, LA\", \"Henderson, NV\", \"Honolulu, HI\", \"Anaheim, CA\", \"Orlando, FL\", \"Lexington, KY\", \"Stockton, CA\", \"Riverside, CA\", \"Irvine, CA\", \"Corpus Christi, TX\", \"Newark, NJ\", \"Santa Ana, CA\", \"Cincinnati, OH\", \"Pittsburgh, PA\", \"Saint Paul, MN\", \"Greensboro, NC\", \"Jersey City, NJ\", \"Durham, NC\", \"Lincoln, NE\", \"North Las Vegas, NV\", \"Plano, TX\", \"Anchorage, AK\", \"Gilbert, AZ\", \"Madison, WI\", \"Reno, NV\", \"Chandler, AZ\", \"St. Louis, MO\", \"Chula Vista, CA\", \"Buffalo, NY\", \"Fort Wayne, IN\", \"Lubbock, TX\", \"St. Petersburg, FL\", \"Toledo, OH\", \"Laredo, TX\", \"Port St. Lucie, FL\", \"Glendale, AZ\", \"Irving, TX\", \"Winston-Salem, NC\", \"Chesapeake, VA\", \"Garland, TX\", \"Scottsdale, AZ\", \"Boise, ID\", \"Hialeah, FL\", \"Frisco, TX\", \"Richmond, VA\", \"Cape Coral, FL\", \"Norfolk, VA\", \"Spokane, WA\", \"Huntsville, AL\", \"Santa Clarita, CA\", \"Tacoma, WA\", \"Fremont, CA\", \"McKinney, TX\", \"San Bernardino, CA\", \"Baton Rouge, LA\", \"Modesto, CA\", \"Fontana, CA\", \"Salt Lake City, UT\", \"Moreno Valley, CA\", \"Des Moines, IA\", \"Worcester, MA\", \"Yonkers, NY\", \"Fayetteville, NC\", \"Sioux Falls, SD\", \"Grand Prairie, TX\", \"Rochester, NY\", \"Tallahassee, FL\", \"Little Rock, AR\", \"Amarillo, TX\", \"Overland Park, KS\", \"Columbus, GA\", \"Augusta, GA\", \"Mobile, AL\", \"Oxnard, CA\", \"Grand Rapids, MI\", \"Peoria, AZ\", \"Vancouver, WA\", \"Knoxville, TN\", \"Birmingham, AL\", \"Montgomery, AL\", \"Providence, RI\", \"Huntington Beach, CA\", \"Brownsville, TX\", \"Chattanooga, TN\", \"Fort Lauderdale, FL\", \"Tempe, AZ\", \"Akron, OH\", \"Glendale, CA\", \"Clarksville, TN\", \"Ontario, CA\", \"Newport News, VA\", \"Elk Grove, CA\", \"Cary, NC\", \"Aurora, IL\", \"Salem, OR\", \"Pembroke Pines, FL\", \"Eugene, OR\", \"Santa Rosa, CA\", \"Rancho Cucamonga, CA\", \"Shreveport, LA\", \"Garden Grove, CA\", \"Oceanside, CA\", \"Fort Collins, CO\", \"Springfield, MO\", \"Murfreesboro, TN\", \"Surprise, AZ\", \"Lancaster, CA\", \"Denton, TX\", \"Roseville, CA\", \"Palmdale, CA\", \"Corona, CA\", \"Salinas, CA\", \"Killeen, TX\", \"Paterson, NJ\", \"Alexandria, VA\", \"Hollywood, FL\", \"Hayward, CA\", \"Charleston, SC\", \"Macon, GA\", \"Lakewood, CO\", \"Sunnyvale, CA\", \"Kansas City, KS\", \"Springfield, MA\", \"Bellevue, WA\", \"Naperville, IL\", \"Joliet, IL\", \"Bridgeport, CT\", \"Mesquite, TX\", \"Pasadena, TX\", \"Olathe, KS\", \"Escondido, CA\", \"Savannah, GA\", \"McAllen, TX\", \"Gainesville, FL\", \"Pomona, CA\", \"Rockford, IL\", \"Thornton, CO\", \"Waco, TX\", \"Visalia, CA\", \"Syracuse, NY\", \"Columbia, SC\", \"Midland, TX\", \"Miramar, FL\", \"Palm Bay, FL\", \"Lakewood,NJ\", \"Jackson, MS\", \"Coral Springs, FL\", \"Victorville, CA\", \"Elizabeth, NJ\", \"Fullerton, CA\", \"Meridian, ID\", \"Torrance, CA\", \"Stamford, CT\", \"West Valley City, UT\", \"Orange, CA\", \"Cedar Rapids, IA\", \"Warren, MI\", \"Hampton, VA\", \"New Haven, CT\", \"Pasadena, CA\", \"Kent, WA\", \"Dayton, OH\", \"Fargo, ND\", \"Lewisville, TX\", \"Carrollton, TX\", \"Round Rock, TX\", \"Sterling Heights, MI\", \"Santa Clara, CA\", \"Norman, OK\", \"Columbia, MO\", \"Abilene, TX\", \"Pearland, TX\", \"Athens, GA\", \"College Station, TX\", \"Clovis, CA\", \"West Palm Beach, FL\", \"Allentown, PA\", \"North Charleston, SC\", \"Simi Valley, CA\", \"Topeka, KS\", \"Wilmington, NC\", \"Lakeland, FL\", \"Thousand Oaks, CA\", \"Concord, CA\", \"Rochester, MN\", \"Vallejo, CA\", \"Ann Arbor, MI\", \"Broken Arrow, OK\", \"Fairfield, CA\", \"Lafayette, LA\", \"Hartford, CT\", \"Arvada, CO\", \"Berkeley, CA\", \"Independence, MO\", \"Billings, MT\", \"Cambridge, MA\", \"Lowell, MA\", \"Odessa, TX\", \"High Point, NC\", \"League City, TX\", \"Antioch, CA\", \"Richardson, TX\", \"Goodyear, AZ\", \"Pompano Beach, FL\", \"Nampa, ID\", \"Menifee, CA\", \"Las Cruces, NM\", \"Clearwater, FL\", \"West Jordan, UT\", \"New Braunfels, TX\", \"Manchester, NH\", \"Miami Gardens, FL\", \"Waterbury, CT\", \"Provo, UT\", \"Evansville, IN\", \"Richmond, CA\", \"Westminster, CO\", \"Elgin, IL\", \"Conroe, TX\", \"Greeley, CO\", \"Lansing, MI\", \"Buckeye, AZ\", \"Tuscaloosa, AL\", \"Allen, TX\", \"Carlsbad, CA\", \"Everett, WA\", \"Springfield,IL\", \"Beaumont, TX\", \"Murrieta, CA\", \"Rio Rancho, NM\", \"Temecula, CA\", \"Concord, NC\", \"Tyler, TX\", \"Davie, FL\", \"South Fulton, GA\", \"Peoria, IL\", \"Sparks, NV\", \"Gresham, OR\", \"Santa Maria, CA\", \"Pueblo, CO\", \"Hillsboro, OR\", \"Edison, NJ\", \"Sugar Land, TX\", \"Ventura, CA\", \"Downey, CA\", \"Costa Mesa, CA\", \"Centennial, CO\", \"Edinburg, TX\", \"Spokane Valley, WA\", \"Jurupa Valley, CA\", \"Bend, OR\", \"West Covina, CA\", \"Boulder, CO\", \"Palm Coast, FL\", \"Lee's Summit, MO\", \"Dearborn, MI\", \"Green Bay, WI\", \"St. George, UT\", \"Woodbridge, NJ\", \"Brockton, MA\", \"Renton, WA\", \"Sandy Springs, GA\", \"Rialto, CA\", \"El Monte, CA\", \"Vacaville, CA\", \"Fishers, IN\", \"South Bend, IN\", \"Carmel, IN\", \"Yuma, AZ\", \"Burbank, CA\", \"Lynn, MA\", \"Quincy, MA\", \"El Cajon, CA\", \"Fayetteville, AR\", \"Suffolk, VA\", \"San Mateo, CA\", \"Chico, CA\", \"Inglewood, CA\", \"Wichita Falls, TX\", \"Boca Raton, FL\", \"Hesperia, CA\", \"Daly City, CA\", \"Clinton, MI\", \"Georgetown, TX\", \"New Bedford, MA\", \"Albany, NY\", \"Davenport, IA\", \"Plantation, FL\", \"Deltona, FL\", \"Federal Way, WA\", \"San Angelo, TX\", \"Tracy, CA\", \"Sunrise, FL\"\n",
        "]\n",
        "\n",
        "CA_PROVINCES = [\"Alberta\", \"British Columbia\", \"Ontario\", \"Quebec\", \"Manitoba\", \"New Brunswick\", \"Newfoundland and Labrador\", \"Nova Scotia\", \"Prince Edward Island\", \"Saskatchewan\"]\n",
        "\n",
        "CA_LOCATIONS = [ \"Winnipeg, Manitoba\", \"Dieppe, New Brunswick\", \"Fredericton, New Brunswick\", \"Greater Lakeburn, New Brunswick\", \"Moncton, New Brunswick\", \"Saint John, New Brunswick\", \"Dartmouth, Nova Scotia\", \"Halifax, Nova Scotia\", \"Aurora, Ontario\", \"Barrie, Ontario\", \"Bowmanville, Ontario\", \"Chatham, Ontario\", \"Collingwood, Ontario\", \"East Gwillimbury, Ontario\", \"Goderich, Ontario\", \"Holland Landing, Ontario\", \"Leamington, Ontario\", \"London, Ontario\", \"Midland, Ontario\", \"Mississauga, Ontario\", \"Newmarket, Ontario\", \"Niagara Falls, Ontario\", \"North York, Ontario\", \"Oakville, Ontario\", \"Ottawa, Ontario\", \"Peterborough, Ontario\", \"Port Carling, Ontario\", \"Queensville, Ontario\", \"Richmond Hill, Ontario\", \"Seguin, Ontario\", \"St. Catharines, Ontario\", \"St. Marys, Ontario\", \"Sudbury, Ontario\", \"Thunder Bay, Ontario\", \"Toronto, Ontario\", \"Waterloo, Ontario\", \"Welland, Ontario\", \"Guelph, Ontario\", \"Scarborough, Ontario\"]\n",
        "\n",
        "LOCATIONS_TO_CHECK = (\n",
        "    [\"United States\"]\n",
        "    + US_STATES\n",
        "    + US_CITIES\n",
        "    + [\"Canada\"]\n",
        "    + CA_PROVINCES\n",
        "    + CA_LOCATIONS\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: CORE LOGIC & HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def get_location_criterion_id(client, location_name):\n",
        "    \"\"\"Converts a location name into a Geo Target Constant Criterion ID.\"\"\"\n",
        "    gtc_service = client.get_service(\"GeoTargetConstantService\")\n",
        "    request = client.get_type(\"SuggestGeoTargetConstantsRequest\")\n",
        "    request.location_names.names.append(location_name)\n",
        "    try:\n",
        "        response = gtc_service.suggest_geo_target_constants(request)\n",
        "        if not response.geo_target_constant_suggestions:\n",
        "            print(f\"    -> ⚠️  Could not find a location criterion for '{location_name}'.\")\n",
        "            return None\n",
        "        return response.geo_target_constant_suggestions[0].geo_target_constant.resource_name\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f\"    -> ❌ Failed to get location ID for '{location_name}': {ex}\")\n",
        "        return None\n",
        "\n",
        "def get_historical_metrics(client, customer_id, keywords, location_criterion):\n",
        "    \"\"\"\n",
        "    Queries the Google Ads API for historical search volume data, starting from a fixed date (Jan 2023)\n",
        "    up to the last full month.\n",
        "    \"\"\"\n",
        "    print(f\"  -> Fetching data for location: '{location_criterion}'...\")\n",
        "\n",
        "    request = client.get_type(\"GenerateKeywordHistoricalMetricsRequest\")\n",
        "    request.customer_id = customer_id\n",
        "    request.keywords = keywords\n",
        "    request.geo_target_constants.append(location_criterion)\n",
        "\n",
        "    # --- UPDATED: Date Range Calculation ---\n",
        "    # The end date is the last day of the previous month.\n",
        "    today = datetime.date.today()\n",
        "    last_full_month_date = today.replace(day=1) - relativedelta(days=1)\n",
        "    end_year = last_full_month_date.year\n",
        "    end_month_enum_value = getattr(client.get_type(\"MonthOfYearEnum\").MonthOfYear, last_full_month_date.strftime('%B').upper())\n",
        "\n",
        "    # The start date is now fixed to January 2023.\n",
        "    start_year = 2023\n",
        "    start_month_enum_value = client.get_type(\"MonthOfYearEnum\").MonthOfYear.JANUARY\n",
        "\n",
        "    print(f\"    -> Requesting historical data from {start_year}-01 to {end_year}-{last_full_month_date.month:02d}.\")\n",
        "\n",
        "    request.historical_metrics_options.year_month_range.start.year = start_year\n",
        "    request.historical_metrics_options.year_month_range.start.month = start_month_enum_value\n",
        "    request.historical_metrics_options.year_month_range.end.year = end_year\n",
        "    request.historical_metrics_options.year_month_range.end.month = end_month_enum_value\n",
        "\n",
        "    try:\n",
        "        time.sleep(1)\n",
        "        keyword_plan_idea_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "        response = keyword_plan_idea_service.generate_keyword_historical_metrics(request=request)\n",
        "\n",
        "        keyword_data = {}\n",
        "        for result in response.results:\n",
        "            monthly_volumes = {}\n",
        "            for s in result.keyword_metrics.monthly_search_volumes:\n",
        "                year = s.year\n",
        "                month = s.month.value - 1\n",
        "\n",
        "                if month == 0:\n",
        "                    month = 12\n",
        "                    year -= 1\n",
        "                elif month > 12:\n",
        "                    month -= 12\n",
        "                    year += 1\n",
        "\n",
        "                month_key = f\"{year}-{month:02d}\"\n",
        "                monthly_volumes[month_key] = s.monthly_searches\n",
        "            keyword_data[result.text] = monthly_volumes\n",
        "\n",
        "        print(f\"    -> Successfully fetched data for {len(keyword_data)} keywords.\")\n",
        "        return keyword_data\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f'    -> ❌ Request failed: {ex.error.code().name}')\n",
        "        for error in ex.failure.errors:\n",
        "            print(f'       Error: {error.message} (Field: {error.location.field_path_elements[0].field_name})')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"    -> ❌ An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def aggregate_data(keyword_data, groups):\n",
        "    \"\"\"\n",
        "    Aggregates search volume by the predefined groups (summing).\n",
        "    This function now uses ALL unique months present in the API response data,\n",
        "    with no upper limit on the number of months.\n",
        "    \"\"\"\n",
        "    print(\"  -> Aggregating data...\")\n",
        "\n",
        "    # 1. Collect ALL unique YYYY-MM strings that actually exist in the API response.\n",
        "    all_unique_months_from_api = set()\n",
        "    for keyword_metrics in keyword_data.values():\n",
        "        for month_key in keyword_metrics.keys():\n",
        "            all_unique_months_from_api.add(month_key)\n",
        "\n",
        "    # 2. Sort these months chronologically. This is crucial for correct sheet ordering.\n",
        "    sorted_api_months = sorted(list(all_unique_months_from_api), key=lambda x: datetime.datetime.strptime(x, '%Y-%m'))\n",
        "\n",
        "    # 3. Use all sorted months. The 25-month limit has been removed.\n",
        "    final_months = sorted_api_months\n",
        "\n",
        "    print(f\"    -> Aggregating across {len(final_months)} total months.\")\n",
        "\n",
        "    # Initialize aggregated_volumes with the definitive month list (oldest to newest)\n",
        "    aggregated_volumes = {group: {month: 0 for month in final_months} for group in groups}\n",
        "\n",
        "    for group_name, keywords_in_group in groups.items():\n",
        "        for keyword in keywords_in_group:\n",
        "            if keyword in keyword_data:\n",
        "                for month, volume in keyword_data[keyword].items():\n",
        "                    # Check if the month is in our list (it always should be)\n",
        "                    if month in final_months:\n",
        "                        aggregated_volumes[group_name][month] += volume\n",
        "\n",
        "    print(\"    -> Aggregation complete.\")\n",
        "    return aggregated_volumes, final_months\n",
        "\n",
        "def update_google_sheet(spreadsheet_url, location_name, aggregated_data, months):\n",
        "    \"\"\"\n",
        "    Updates the sheet by duplicating a template, clearing A1:X#, and pasting new data.\n",
        "    Now dynamically adjusts for up to 25 months.\n",
        "    \"\"\"\n",
        "    print(f\"  -> Preparing to update sheet for '{location_name}'...\")\n",
        "    max_retries = 5\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "            try:\n",
        "                template_sheet = spreadsheet.worksheet(\"Sheet1\")\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                print(\"    -> ❌ CRITICAL ERROR: The template 'Sheet1' was not found.\"); return\n",
        "\n",
        "            try:\n",
        "                worksheet = spreadsheet.worksheet(location_name)\n",
        "                print(f\"    -> Found existing worksheet '{location_name}'.\")\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                print(f\"    -> Worksheet '{location_name}' not found. Duplicating from 'Sheet1' at end of list...\")\n",
        "\n",
        "                source_sheet_id = template_sheet.id\n",
        "                insert_index = len(spreadsheet.worksheets())\n",
        "\n",
        "                requests = [{\"duplicateSheet\": {\"sourceSheetId\": source_sheet_id, \"newSheetName\": location_name, \"insertSheetIndex\": insert_index}}]\n",
        "                spreadsheet.batch_update({\"requests\": requests})\n",
        "\n",
        "                worksheet = spreadsheet.worksheet(location_name)\n",
        "                print(f\"    -> New worksheet '{location_name}' created and positioned at the end.\")\n",
        "\n",
        "            group_names = list(KEYWORD_GROUPS.keys())\n",
        "            header_row = ['Keyword Group'] + months\n",
        "            volume_rows = [[group] + [aggregated_data[group].get(month, 0) for month in months] for group in group_names]\n",
        "\n",
        "            data_to_paste = [header_row] + volume_rows\n",
        "\n",
        "            num_rows_to_clear = len(data_to_paste)\n",
        "            num_cols_to_clear = len(data_to_paste[0])\n",
        "            clear_end_cell = rowcol_to_a1(num_rows_to_clear, num_cols_to_clear)\n",
        "            clear_range = f'A1:{clear_end_cell}'\n",
        "\n",
        "            print(f\"    -> Clearing target range: {clear_range}...\")\n",
        "            worksheet.batch_clear([clear_range])\n",
        "            print(\"    -> Pasting monthly search volume data into A1.\")\n",
        "            worksheet.update('A1', data_to_paste, value_input_option='USER_ENTERED')\n",
        "\n",
        "            worksheet.spreadsheet.batch_update({\n",
        "                \"requests\": [{\"autoResizeDimensions\": {\"dimensions\": {\"sheetId\": worksheet.id, \"dimension\": \"COLUMNS\", \"startIndex\": 0, \"endIndex\": num_cols_to_clear}}}]\n",
        "            })\n",
        "\n",
        "            print(f\"    -> ✅ Sheet '{location_name}' updated successfully!\")\n",
        "            return\n",
        "\n",
        "        except gspread.exceptions.APIError as e:\n",
        "            if e.response.status_code in [429, 500]:\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"    -> ⚠️ APIError (Status {e.response.status_code}). Retrying in {wait_time} seconds (attempt {attempt + 1}/{max_retries})...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"    -> ❌ An unrecoverable API error occurred while updating the Google Sheet: {e}\")\n",
        "                return\n",
        "        except Exception as e:\n",
        "            print(f\"    -> ❌ An unexpected error occurred while updating the Google Sheet: {e}\")\n",
        "            return\n",
        "\n",
        "    print(f\"    -> ❌ Failed to update sheet '{location_name}' after {max_retries} attempts.\")\n",
        "\n",
        "def delete_all_except_specified_sheets(spreadsheet_url, sheets_to_keep):\n",
        "    \"\"\"\n",
        "    Deletes all worksheets in a Google Sheet except those specified in sheets_to_keep.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Cleaning up Spreadsheet: Deleting unnecessary sheets ---\")\n",
        "    time.sleep(1)\n",
        "    try:\n",
        "        spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "        all_worksheets = spreadsheet.worksheets()\n",
        "\n",
        "        sheets_deleted = []\n",
        "        for worksheet in all_worksheets:\n",
        "            if worksheet.title not in sheets_to_keep:\n",
        "                print(f\"  -> Deleting sheet: '{worksheet.title}'...\")\n",
        "                time.sleep(.5)\n",
        "                spreadsheet.del_worksheet(worksheet)\n",
        "                sheets_deleted.append(worksheet.title)\n",
        "            else:\n",
        "                print(f\"  -> Keeping sheet: '{worksheet.title}'\")\n",
        "\n",
        "        if sheets_deleted:\n",
        "            print(f\"✅ Successfully deleted: {', '.join(sheets_deleted)}\")\n",
        "        else:\n",
        "            print(\"  -> No sheets to delete (all specified sheets were kept).\")\n",
        "\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        print(f\"❌ Error: Spreadsheet not found at URL: {spreadsheet_url}. Cannot clean up.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred during sheet cleanup: {e}\")\n",
        "\n",
        "# --- NEW: Google Docs Logging Functions ---\n",
        "# --- CORRECTED: Google Docs Logging Functions ---\n",
        "def get_doc_content_length(service, document_id):\n",
        "    \"\"\"\n",
        "    Gets the insertion point at the end of the Google Doc.\n",
        "    The API's endIndex is an exclusive upper bound, so we subtract 1\n",
        "    to get the last valid insertion index.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = service.documents().get(documentId=document_id, fields='body(content(endIndex))').execute()\n",
        "        # FIX: Subtract 1 from the endIndex to get the correct insertion point.\n",
        "        return doc['body']['content'][-1]['endIndex'] - 1\n",
        "    except HttpError as e:\n",
        "        print(f\"    -> ❌ Error getting document length: {e}\")\n",
        "        # Fallback for empty or erroring doc: 1 is the beginning of the document.\n",
        "        return 1\n",
        "\n",
        "def batch_update_doc(service, document_id, requests):\n",
        "    \"\"\"Applies a batch of update requests to the Google Doc.\"\"\"\n",
        "    if not requests:\n",
        "        print(\"  -> No log data to send to Google Docs.\")\n",
        "        return\n",
        "    try:\n",
        "        service.documents().batchUpdate(\n",
        "            documentId=document_id,\n",
        "            body={'requests': requests}\n",
        "        ).execute()\n",
        "        print(\"  -> ✅ Successfully wrote logs to Google Doc.\")\n",
        "    except HttpError as e:\n",
        "        print(f\"    -> ❌ Error writing to Google Doc: {e}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: MAIN EXECUTION (UPDATED WITH H2 LOG HEADER)\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    print(\"\\n--- Starting Main Execution ---\")\n",
        "    if not google_ads_client or not docs_service:\n",
        "        print(\"\\n❌ Halting execution: A required client (Google Ads or Docs) is not initialized.\")\n",
        "        return\n",
        "\n",
        "    all_keywords = [kw for group in KEYWORD_GROUPS.values() for kw in group]\n",
        "    if not all_keywords:\n",
        "        print(\"No keywords found. Please add keywords.\")\n",
        "        return\n",
        "\n",
        "    # --- Initialize Logging with H2 Header ---\n",
        "    log_requests = []\n",
        "    run_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    # We add a newline at the end for spacing after the header.\n",
        "    header_text = f\"{run_timestamp}\\n\"\n",
        "\n",
        "    try:\n",
        "        # Get the starting position for our new log entries.\n",
        "        insert_at_index = get_doc_content_length(docs_service, DOCS_LOG_ID)-1\n",
        "        if insert_at_index > 1: # Add a newline if we aren't at the very top\n",
        "             header_text = \"\\n\" + header_text\n",
        "             insert_at_index += 1\n",
        "\n",
        "\n",
        "        # 1. First request: Insert the header text.\n",
        "        log_requests.append({\n",
        "            'insertText': {\n",
        "                'location': {'index': insert_at_index},\n",
        "                'text': header_text\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # 2. Second request: Apply the 'Heading 2' style to the text we just inserted.\n",
        "        log_requests.append({\n",
        "            'updateParagraphStyle': {\n",
        "                'range': {\n",
        "                    'startIndex': insert_at_index,\n",
        "                    'endIndex': insert_at_index + len(header_text)\n",
        "                },\n",
        "                'paragraphStyle': {\n",
        "                    'namedStyleType': 'HEADING_2'\n",
        "                },\n",
        "                # Specify the field we are updating.\n",
        "                'fields': 'namedStyleType'\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Send the batch containing both requests.\n",
        "        batch_update_doc(docs_service, DOCS_LOG_ID, log_requests)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  -> ❌ Could not write initial header to Google Doc: {e}\")\n",
        "\n",
        "\n",
        "    # This list will hold the log strings for the current batch.\n",
        "    log_batch = []\n",
        "\n",
        "    # Optional: Enable sheet cleanup\n",
        "    sheets_to_preserve = ['Sheet1', 'StateSummary']\n",
        "#    delete_all_except_specified_sheets(SPREADSHEET_URL, sheets_to_preserve)\n",
        "\n",
        "    customer_id = credentials['login_customer_id'].replace(\"-\", \"\")\n",
        "\n",
        "    for i, location_name_str in enumerate(LOCATIONS_TO_CHECK):\n",
        "\n",
        "        # Determine the fully qualified location name for the API\n",
        "        if location_name_str in US_CITIES or location_name_str in CA_LOCATIONS:\n",
        "            # Already fully qualified, e.g., \"Los Angeles, CA\" or \"Toronto, Ontario\"\n",
        "            qualified_location_name = location_name_str\n",
        "        elif location_name_str in US_STATES:\n",
        "            # e.g., \"California\" -> \"California, United States\"\n",
        "            qualified_location_name = f\"{location_name_str}, United States\"\n",
        "        elif location_name_str in CA_PROVINCES:\n",
        "            # e.g., \"Ontario\" -> \"Ontario, Canada\"\n",
        "            qualified_location_name = f\"{location_name_str}, Canada\"\n",
        "        elif location_name_str == \"United States\" or location_name_str == \"Canada\":\n",
        "            # Whole country\n",
        "            qualified_location_name = location_name_str\n",
        "        else:\n",
        "            # Should not be reached, but as a fallback, use the name as is\n",
        "            qualified_location_name = location_name_str\n",
        "\n",
        "        print(f\"\\n--- Processing Location ({i+1}/{len(LOCATIONS_TO_CHECK)}): {location_name_str} (as '{qualified_location_name}') ---\")\n",
        "\n",
        "        log_entry = f\"Processing: {location_name_str}... \"\n",
        "\n",
        "        location_criterion_resource_name = get_location_criterion_id(google_ads_client, qualified_location_name)\n",
        "\n",
        "        if location_criterion_resource_name:\n",
        "            keyword_metrics = get_historical_metrics(google_ads_client, customer_id, all_keywords, location_criterion_resource_name)\n",
        "            if keyword_metrics:\n",
        "                aggregated_data, months = aggregate_data(keyword_metrics, KEYWORD_GROUPS)\n",
        "                update_google_sheet(SPREADSHEET_URL, location_name_str, aggregated_data, months)\n",
        "                log_entry += \"✅ Success\\n\"\n",
        "            else:\n",
        "                print(f\"  -> Skipping sheet update for '{location_name_str}' due to data fetch failure.\")\n",
        "                log_entry += \"❌ Failed (Data Fetch Error)\\n\"\n",
        "        else:\n",
        "            print(f\"  -> Skipping '{location_name_str}' due to invalid location name.\")\n",
        "            log_entry += \"❌ Failed (Invalid Location)\\n\"\n",
        "\n",
        "        log_batch.append(log_entry)\n",
        "\n",
        "        if len(log_batch) >= 10:\n",
        "            print(\"\\n--- Writing batch of 10 logs to Google Doc ---\")\n",
        "            full_log_text = \"\".join(log_batch)\n",
        "            try:\n",
        "                insert_at_index = get_doc_content_length(docs_service, DOCS_LOG_ID)\n",
        "                requests = [{'insertText': {'location': {'index': insert_at_index}, 'text': full_log_text}}]\n",
        "                batch_update_doc(docs_service, DOCS_LOG_ID, requests)\n",
        "                log_batch.clear()\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ❌ Could not write log batch to Google Doc: {e}\")\n",
        "\n",
        "    if log_batch:\n",
        "        print(\"\\n--- Writing final batch of logs to Google Doc ---\")\n",
        "        full_log_text = \"\".join(log_batch)\n",
        "        try:\n",
        "            insert_at_index = get_doc_content_length(docs_service, DOCS_LOG_ID)\n",
        "            requests = [{'insertText': {'location': {'index': insert_at_index}, 'text': full_log_text}}]\n",
        "            batch_update_doc(docs_service, DOCS_LOG_ID, requests)\n",
        "        except Exception as e:\n",
        "            print(f\"  -> ❌ Could not write final log batch to Google Doc: {e}\")\n",
        "\n",
        "    print(\"\\n\\n--- All operations complete. ---\")\n",
        "    print(f\"View the results here: {SPREADSHEET_URL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GRsgl9pdh-8T",
        "outputId": "b2f66bc2-f7b3-4321-9a8b-86a337a5c737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Main Execution ---\n",
            "  -> ✅ Successfully wrote logs to Google Doc.\n",
            "\n",
            "--- Processing Location (1/447): United States (as 'United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/2840'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'United States'...\n",
            "    -> Found existing worksheet 'United States'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2292602410.py:257: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  worksheet.update('A1', data_to_paste, value_input_option='USER_ENTERED')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    -> ✅ Sheet 'United States' updated successfully!\n",
            "\n",
            "--- Processing Location (2/447): Alabama (as 'Alabama, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21133'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Alabama'...\n",
            "    -> Found existing worksheet 'Alabama'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Alabama' updated successfully!\n",
            "\n",
            "--- Processing Location (3/447): Alaska (as 'Alaska, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21132'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Alaska'...\n",
            "    -> Found existing worksheet 'Alaska'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Alaska' updated successfully!\n",
            "\n",
            "--- Processing Location (4/447): Arizona (as 'Arizona, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21136'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Arizona'...\n",
            "    -> Found existing worksheet 'Arizona'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Arizona' updated successfully!\n",
            "\n",
            "--- Processing Location (5/447): Arkansas (as 'Arkansas, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21135'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Arkansas'...\n",
            "    -> Found existing worksheet 'Arkansas'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Arkansas' updated successfully!\n",
            "\n",
            "--- Processing Location (6/447): California (as 'California, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21137'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'California'...\n",
            "    -> Found existing worksheet 'California'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'California' updated successfully!\n",
            "\n",
            "--- Processing Location (7/447): Colorado (as 'Colorado, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21138'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Colorado'...\n",
            "    -> Found existing worksheet 'Colorado'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Colorado' updated successfully!\n",
            "\n",
            "--- Processing Location (8/447): Connecticut (as 'Connecticut, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21139'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Connecticut'...\n",
            "    -> Found existing worksheet 'Connecticut'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Connecticut' updated successfully!\n",
            "\n",
            "--- Processing Location (9/447): Delaware (as 'Delaware, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21141'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Delaware'...\n",
            "    -> Found existing worksheet 'Delaware'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Delaware' updated successfully!\n",
            "\n",
            "--- Processing Location (10/447): Florida (as 'Florida, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21142'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Florida'...\n",
            "    -> Found existing worksheet 'Florida'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Florida' updated successfully!\n",
            "\n",
            "--- Writing batch of 10 logs to Google Doc ---\n",
            "  -> ✅ Successfully wrote logs to Google Doc.\n",
            "\n",
            "--- Processing Location (11/447): Georgia (as 'Georgia, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21143'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Georgia'...\n",
            "    -> Found existing worksheet 'Georgia'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n",
            "    -> ✅ Sheet 'Georgia' updated successfully!\n",
            "\n",
            "--- Processing Location (12/447): Hawaii (as 'Hawaii, United States') ---\n",
            "  -> Fetching data for location: 'geoTargetConstants/21144'...\n",
            "    -> Requesting historical data from 2023-01 to 2025-08.\n",
            "    -> Successfully fetched data for 488 keywords.\n",
            "  -> Aggregating data...\n",
            "    -> Aggregating across 31 total months.\n",
            "    -> Aggregation complete.\n",
            "  -> Preparing to update sheet for 'Hawaii'...\n",
            "    -> Found existing worksheet 'Hawaii'.\n",
            "    -> Clearing target range: A1:AF6...\n",
            "    -> Pasting monthly search volume data into A1.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4SzSYachdXW"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 6: CUSTOM FUNCTIONS (State Summary - WITH CHANGE ANALYSIS)\n",
        "# ==============================================================================\n",
        "\n",
        "# IMPORTANT: Ensure SPREADSHEET_URL, KEYWORD_GROUPS, and US_STATES\n",
        "# are defined in a previously executed cell before running this.\n",
        "\n",
        "def create_state_summary_sheet(spreadsheet_url):\n",
        "    \"\n",
        "    Creates/updates a 'StateSummary' sheet with states as rows and months as columns.\n",
        "    It now intelligently appends summary columns for 1, 3, 6, 12, and 24-month\n",
        "    change and percentage change, based on the available data.\n",
        "    \"\n",
        "    print(\"\\n--- Creating/Updating State Summary Sheet (with Change Analysis) ---\")\n",
        "    try:\n",
        "        spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "\n",
        "        try:\n",
        "            summary_worksheet = spreadsheet.worksheet(\"StateSummary\")\n",
        "            print(\"  -> Found existing 'StateSummary' worksheet.\")\n",
        "        except gspread.exceptions.WorksheetNotFound:\n",
        "            print(\"  -> 'StateSummary' worksheet not found. Creating it...\")\n",
        "            summary_worksheet = spreadsheet.add_worksheet(title=\"StateSummary\", rows=\"100\", cols=\"70\", index=1)\n",
        "            print(\"  -> New 'StateSummary' worksheet created.\")\n",
        "\n",
        "        months = []\n",
        "        try:\n",
        "            sample_sheet = spreadsheet.worksheet(\"United States\")\n",
        "            months = sample_sheet.row_values(1)[1:]\n",
        "            if not months: raise ValueError(\"'United States' sheet is empty.\")\n",
        "            print(f\"  -> Successfully loaded {len(months)} month headers from 'United States' sheet.\")\n",
        "        except (gspread.exceptions.WorksheetNotFound, ValueError, IndexError) as e:\n",
        "            print(f\"  -> WARNING: Could not get month headers ({e}). Generating fallback.\")\n",
        "            start_date = datetime.date(2023, 1, 1)\n",
        "            end_date = datetime.date.today().replace(day=1) - relativedelta(days=1)\n",
        "            current_date = start_date\n",
        "            while current_date <= end_date:\n",
        "                months.append(current_date.strftime('%Y-%m'))\n",
        "                current_date += relativedelta(months=1)\n",
        "            print(f\"  -> Generated {len(months)} fallback month headers.\")\n",
        "\n",
        "        # --- Base data setup ---\n",
        "        header_row = ['State'] + months\n",
        "        data_rows = []\n",
        "        all_sheet_titles = [s.title for s in spreadsheet.worksheets()]\n",
        "        existing_state_sheets = [s for s in US_STATES if s in all_sheet_titles]\n",
        "\n",
        "        if not existing_state_sheets:\n",
        "            print(\"  -> No state-specific sheets found. Aborting summary creation.\")\n",
        "            return\n",
        "\n",
        "        for state_name in existing_state_sheets:\n",
        "            formula_row = [state_name]\n",
        "            for j in range(len(months)):\n",
        "                col_index = j + 2\n",
        "                a1_notation = rowcol_to_a1(1, col_index)\n",
        "                source_col_letter = ''.join(filter(str.isalpha, a1_notation))\n",
        "                formula = f\"='{state_name}'!{source_col_letter}7\"\n",
        "                formula_row.append(formula)\n",
        "            data_rows.append(formula_row)\n",
        "\n",
        "        # --- NEW: Add summary columns ---\n",
        "        summary_periods = [('1m', 1), ('3m', 3), ('6m', 6), ('12m', 12), ('24m', 24)]\n",
        "        num_months = len(months)\n",
        "        last_month_col_index = 1 + num_months\n",
        "        last_month_col_letter = ''.join(filter(str.isalpha, rowcol_to_a1(1, last_month_col_index)))\n",
        "\n",
        "        percentage_format_requests = []\n",
        "\n",
        "        for label, period_months in summary_periods:\n",
        "            # Only add summary columns if we have enough historical data\n",
        "            if num_months > period_months:\n",
        "                print(f\"  -> Adding {label} summary columns.\")\n",
        "                # Define new headers\n",
        "                change_header = f\"{label} Change\"\n",
        "                pct_change_header = f\"{label} % Change\"\n",
        "                header_row.extend([change_header, pct_change_header])\n",
        "\n",
        "                # Get the column for the base month (e.g., 12 months ago)\n",
        "                base_month_col_index = last_month_col_index - period_months\n",
        "                base_month_col_letter = ''.join(filter(str.isalpha, rowcol_to_a1(1, base_month_col_index)))\n",
        "\n",
        "                # Get the column for the \"Change\" value we are about to add\n",
        "                change_col_index = len(header_row) - 1\n",
        "                change_col_letter = ''.join(filter(str.isalpha, rowcol_to_a1(1, change_col_index)))\n",
        "\n",
        "                # Add the formulas to each data row\n",
        "                for i, row in enumerate(data_rows):\n",
        "                    sheet_row_num = i + 2 # +2 because sheet is 1-indexed and has a header\n",
        "\n",
        "                    # Formula for absolute change: =(LastMonth - BaseMonth)\n",
        "                    change_formula = f\"={last_month_col_letter}{sheet_row_num}-{base_month_col_letter}{sheet_row_num}\"\n",
        "\n",
        "                    # Formula for % change: =IFERROR(Change / BaseMonth, 0)\n",
        "                    pct_change_formula = f\"=IFERROR({change_col_letter}{sheet_row_num}/{base_month_col_letter}{sheet_row_num}, 0)\"\n",
        "\n",
        "                    row.extend([change_formula, pct_change_formula])\n",
        "\n",
        "                # Prepare a request to format the % change column\n",
        "                pct_column_index = len(header_row) - 1 # 0-indexed column\n",
        "                percentage_format_requests.append({\n",
        "                    \"repeatCell\": {\n",
        "                        \"range\": {\"sheetId\": summary_worksheet.id, \"startColumnIndex\": pct_column_index, \"endColumnIndex\": pct_column_index + 1},\n",
        "                        \"cell\": {\"userEnteredFormat\": {\"numberFormat\": {\"type\": \"PERCENT\", \"pattern\": \"0.00%\"}}},\n",
        "                        \"fields\": \"userEnteredFormat.numberFormat\"\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # --- Combine all rows for final pasting ---\n",
        "        final_rows_to_paste = [header_row] + data_rows\n",
        "\n",
        "        # --- Update the sheet ---\n",
        "        print(f\"  -> Preparing to update sheet with {len(final_rows_to_paste)} rows and {len(header_row)} columns.\")\n",
        "        summary_worksheet.clear()\n",
        "        summary_worksheet.update('A1', final_rows_to_paste, value_input_option='USER_ENTERED')\n",
        "\n",
        "        # --- Apply all formatting requests and resize ---\n",
        "        update_requests = [{\n",
        "            \"autoResizeDimensions\": {\"dimensions\": {\"sheetId\": summary_worksheet.id, \"dimension\": \"COLUMNS\", \"startIndex\": 0, \"endIndex\": len(header_row)}}\n",
        "        }] + percentage_format_requests\n",
        "\n",
        "        summary_worksheet.spreadsheet.batch_update({\"requests\": update_requests})\n",
        "\n",
        "        print(\"✅ 'StateSummary' sheet updated successfully with change analysis!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred while creating/updating the State Summary Sheet: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz2r7mJthzrh",
        "outputId": "25a310b8-dc81-4a15-9fb2-c06b8afd7edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Creating/Updating State Summary Sheet (with Change Analysis) ---\n",
            "  -> Found existing 'StateSummary' worksheet.\n",
            "  -> Successfully loaded 31 month headers from 'United States' sheet.\n",
            "  -> Adding 1m summary columns.\n",
            "  -> Adding 3m summary columns.\n",
            "  -> Adding 6m summary columns.\n",
            "  -> Adding 12m summary columns.\n",
            "  -> Adding 24m summary columns.\n",
            "  -> Preparing to update sheet with 51 rows and 42 columns.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1105598732.py:115: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  summary_worksheet.update('A1', final_rows_to_paste, value_input_option='USER_ENTERED')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 'StateSummary' sheet updated successfully with change analysis!\n"
          ]
        }
      ],
      "source": [
        "# You would call this function in your main() after all individual location\n",
        "# sheets have been processed. Example call in main():\n",
        "\n",
        "create_state_summary_sheet(SPREADSHEET_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu6arkdQezBY"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 7: STANDALONE TEST FUNCTION (Save Raw API Response)\n",
        "# ==============================================================================\n",
        "\n",
        "# IMPORTANT: This function assumes your Google Ads API credentials\n",
        "# (ADS_DEVELOPER_TOKEN, ADS_CLIENT_ID, etc.) are already set up in Colab's Secrets\n",
        "# manager, as described in Section 2 of your main script.\n",
        "\n",
        "# --- NEW/UPDATED IMPORTS REQUIRED ---\n",
        "# No more json_format import needed for this simplified version\n",
        "import json # Still needed if you use it elsewhere, but not for this specific dump\n",
        "import datetime # Needed for date calculations\n",
        "# ------------------------------------\n",
        "\n",
        "def test_api_response_and_save(test_keyword: str, test_location_name: str, output_filename=\"raw_api_test_response.txt\"): # Changed default extension to .txt\n",
        "    \"\n",
        "    Makes a single Google Ads API call for a specific keyword and location,\n",
        "    and saves the raw API response's string representation to a text file for debugging.\n",
        "\n",
        "    Args:\n",
        "        test_keyword (str): The single keyword to test.\n",
        "        test_location_name (str): The location name (e.g., \"United States\", \"Kansas\").\n",
        "        output_filename (str): The name of the text file to save the response to.\n",
        "    \"\n",
        "    print(f\"\\n--- Running Standalone API Test for Keyword: '{test_keyword}' in '{test_location_name}' ---\")\n",
        "\n",
        "    google_ads_client_local = None\n",
        "    try:\n",
        "        credentials_local = {\n",
        "            \"developer_token\": userdata.get('ADS_DEVELOPER_TOKEN'),\n",
        "            \"client_id\": userdata.get('ADS_CLIENT_ID'),\n",
        "            \"client_secret\": userdata.get('ADS_CLIENT_SECRET'),\n",
        "            \"refresh_token\": userdata.get('ADS_REFRESH_TOKEN'),\n",
        "            \"login_customer_id\": userdata.get('ADS_LOGIN_CUSTOMER_ID'),\n",
        "            \"use_proto_plus\": True\n",
        "        }\n",
        "        google_ads_client_local = GoogleAdsClient.load_from_dict(credentials_local)\n",
        "        print(\"  -> Google Ads client initialized for test.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error initializing Google Ads client for test: {e}. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    customer_id_for_api = credentials_local['login_customer_id'].replace(\"-\", \"\")\n",
        "\n",
        "    # Get location criterion ID\n",
        "    gtc_service = google_ads_client_local.get_service(\"GeoTargetConstantService\")\n",
        "    location_request = google_ads_client_local.get_type(\"SuggestGeoTargetConstantsRequest\")\n",
        "    location_request.location_names.names.append(test_location_name)\n",
        "\n",
        "    location_criterion_resource_name = None\n",
        "    try:\n",
        "        location_response = gtc_service.suggest_geo_target_constants(location_request)\n",
        "        if location_response.geo_target_constant_suggestions:\n",
        "            location_criterion_resource_name = location_response.geo_target_constant_suggestions[0].geo_target_constant.resource_name\n",
        "            print(f\"  -> Found Criterion ID for '{test_location_name}': {location_criterion_resource_name}\")\n",
        "        else:\n",
        "            print(f\"  -> ⚠️ Could not find a location criterion for '{test_location_name}'. Cannot proceed.\")\n",
        "            return\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f\"  ❌ Failed to get location ID for '{test_location_name}': {ex.error.code().name}. Cannot proceed.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ An unexpected error occurred while getting location ID: {e}. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    # Define the historical metrics options to get the desired 25-month date range\n",
        "    today = datetime.date.today()\n",
        "    last_full_month_date = today.replace(day=1) - datetime.timedelta(days=1)\n",
        "    end_year = last_full_month_date.year\n",
        "    end_month_enum_value = getattr(google_ads_client_local.get_type(\"MonthOfYearEnum\").MonthOfYear, last_full_month_date.strftime('%B').upper())\n",
        "\n",
        "    start_dt = datetime.datetime(end_year, last_full_month_date.month, 1)\n",
        "    for _ in range(24): # Go back 24 months from the end date for a total of 25 months\n",
        "        year = start_dt.year\n",
        "        month = start_dt.month - 1\n",
        "        if month == 0:\n",
        "            month = 12\n",
        "            year -= 1\n",
        "        start_dt = datetime.datetime(year, month, 1)\n",
        "\n",
        "    start_year = start_dt.year\n",
        "    start_month_enum_value = getattr(google_ads_client_local.get_type(\"MonthOfYearEnum\").MonthOfYear, start_dt.strftime('%B').upper())\n",
        "\n",
        "    print(f\"  -> Requesting historical data from {start_year}-{start_dt.month:02d} to {end_year}-{last_full_month_date.month:02d} (total 25 months).\")\n",
        "\n",
        "    api_request = google_ads_client_local.get_type(\"GenerateKeywordHistoricalMetricsRequest\")\n",
        "    api_request.customer_id = customer_id_for_api\n",
        "    api_request.keywords.append(test_keyword)\n",
        "    api_request.geo_target_constants.append(location_criterion_resource_name)\n",
        "\n",
        "    api_request.historical_metrics_options.year_month_range.start.year = start_year\n",
        "    api_request.historical_metrics_options.year_month_range.start.month = start_month_enum_value\n",
        "    api_request.historical_metrics_options.year_month_range.end.year = end_year\n",
        "    api_request.historical_metrics_options.year_month_range.end.month = end_month_enum_value\n",
        "\n",
        "    try:\n",
        "        keyword_plan_idea_service = google_ads_client_local.get_service(\"KeywordPlanIdeaService\")\n",
        "        print(f\"  -> Sending API request for keyword '{test_keyword}' in '{test_location_name}'...\")\n",
        "        raw_api_response = keyword_plan_idea_service.generate_keyword_historical_metrics(request=api_request)\n",
        "\n",
        "        # --- FIX: Directly convert the raw response object to a string and save ---\n",
        "        # This will save the __str__ representation (the one you saw printed to console).\n",
        "        # It won't be proper JSON, but it will be the raw API response content.\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(str(raw_api_response)) # Convert to string and write directly\n",
        "\n",
        "        print(f\"  ✅ Raw API response (string representation) successfully saved to '{output_filename}'.\")\n",
        "        print(f\"     You can view this file in Colab's file browser (left sidebar -> folder icon).\")\n",
        "        print(\"     NOTE: This content is the object's string representation, not formatted JSON.\")\n",
        "        print(\"\\n--- Test complete. ---\")\n",
        "\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f\"  ❌ API Request failed: {ex.error.code().name}.\")\n",
        "        for error in ex.failure.errors:\n",
        "            print(f\"    -> Error message: {error.message}\")\n",
        "            if error.location:\n",
        "                for field_path_element in error.location.field_path_elements:\n",
        "                    print(f\"      -> On field: {field_path_element.field_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "# --- Example calls (uncomment to run in the cell) ---\n",
        "# test_api_response_and_save(\"self storage unit\", \"United States\")\n",
        "# test_api_response_and_save(\"storage units\", \"Kansas\")\n",
        "# test_api_response_and_save(\"drive up storage\", \"Overland Park, Kansas\", \"drive_up_op_raw.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIFkCR9XLqJ5"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 8: TABLE OF CONTENTS GENERATION\n",
        "# ==============================================================================\n",
        "import gspread.utils\n",
        "from gspread_formatting import *\n",
        "\n",
        "def create_table_of_contents(spreadsheet_url):\n",
        "    \"\"\"\n",
        "    Creates or updates a 'Table of Contents' sheet in the specified Google Sheet.\n",
        "\n",
        "    The function gathers all existing sheets, categorizes them based on the\n",
        "    pre-defined lists in the configuration, and generates a 4-column layout\n",
        "    of hyperlinks to each sheet, with merged headers for each category.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Table of Contents Generation ---\")\n",
        "    try:\n",
        "        spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "        print(\"  -> Successfully opened the spreadsheet.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  -> ❌ Failed to open spreadsheet: {e}\")\n",
        "        return\n",
        "\n",
        "    # 1. Gather all existing sheets and their properties (title and id)\n",
        "    all_sheets = spreadsheet.worksheets()\n",
        "    sheet_info = {sheet.title: sheet.id for sheet in all_sheets}\n",
        "    print(f\"  -> Found {len(all_sheets)} existing sheets.\")\n",
        "\n",
        "    # 2. Define the order and content of the ToC sections\n",
        "    #    The key is the header that will be displayed.\n",
        "    #    The value is the list of locations for that section.\n",
        "    sections = {\n",
        "        \"United States\": [\"United States\"],\n",
        "        \"U.S. States\": US_STATES,\n",
        "        \"U.S. Cities\": US_CITIES,\n",
        "        \"Canada\": [\"Canada\"],\n",
        "        \"Canadian Provinces\": CA_PROVINCES,\n",
        "        \"Canadian Locations\": CA_LOCATIONS,\n",
        "    }\n",
        "\n",
        "    # 3. Categorize the existing sheets into the defined sections\n",
        "    organized_sheets = {header: [] for header in sections.keys()}\n",
        "    for sheet_name in sheet_info.keys():\n",
        "        if sheet_name == \"Table of Contents\" or sheet_name == \"Sheet1\":\n",
        "            continue # Skip the template and the ToC itself\n",
        "\n",
        "        for header, location_list in sections.items():\n",
        "            if sheet_name in location_list:\n",
        "                organized_sheets[header].append(sheet_name)\n",
        "                break\n",
        "\n",
        "    # 4. Prepare data and formatting requests for the batch update\n",
        "    data_to_write = []\n",
        "    formatting_requests = []\n",
        "    current_row_index = 1 # gspread is 1-indexed for ranges\n",
        "\n",
        "    # Define cell formats\n",
        "    header_format = CellFormat(\n",
        "        backgroundColor=Color(0.9, 0.9, 0.9), # Light grey\n",
        "        textFormat=TextFormat(bold=True, fontSize=12),\n",
        "        horizontalAlignment='CENTER'\n",
        "    )\n",
        "\n",
        "    for header, sheet_names in organized_sheets.items():\n",
        "        if not sheet_names:\n",
        "            continue # Skip sections that have no corresponding sheets\n",
        "\n",
        "        # --- Add Header Row ---\n",
        "        data_to_write.append([header])\n",
        "        header_range = f'A{current_row_index}:D{current_row_index}'\n",
        "        formatting_requests.append(\n",
        "            {'mergeCells': {\n",
        "                'range': {'sheetId': None, 'startRowIndex': current_row_index - 1, 'endRowIndex': current_row_index, 'startColumnIndex': 0, 'endColumnIndex': 4},\n",
        "                'mergeType': 'MERGE_ALL'\n",
        "            }}\n",
        "        )\n",
        "        formatting_requests.append(\n",
        "            {'repeatCell': {\n",
        "                'range': {'sheetId': None, 'startRowIndex': current_row_index - 1, 'endRowIndex': current_row_index, 'startColumnIndex': 0, 'endColumnIndex': 4},\n",
        "                'cell': {'userEnteredFormat': {'textFormat': {'bold': True}, 'horizontalAlignment': 'CENTER'}},\n",
        "                'fields': 'userEnteredFormat(textFormat,horizontalAlignment)'\n",
        "            }}\n",
        "        )\n",
        "        current_row_index += 1\n",
        "\n",
        "        # --- Add Hyperlink Rows (in a 4-column grid) ---\n",
        "        sheet_names.sort() # Sort alphabetically\n",
        "        num_columns = 4\n",
        "        for i in range(0, len(sheet_names), num_columns):\n",
        "            row_data = []\n",
        "            chunk = sheet_names[i:i + num_columns]\n",
        "            for sheet_name in chunk:\n",
        "                gid = sheet_info[sheet_name]\n",
        "                # Formula to create a hyperlink to another sheet in the same document\n",
        "                hyperlink_formula = f'=HYPERLINK(\"#gid={gid}\",\"{sheet_name}\")'\n",
        "                row_data.append(hyperlink_formula)\n",
        "            # Pad the row with empty strings if it's not full\n",
        "            while len(row_data) < num_columns:\n",
        "                row_data.append(\"\")\n",
        "            data_to_write.append(row_data)\n",
        "        current_row_index += len(range(0, len(sheet_names), num_columns))\n",
        "\n",
        "\n",
        "        # --- Add a blank row for spacing ---\n",
        "        data_to_write.append([\"\", \"\", \"\", \"\"])\n",
        "        current_row_index += 1\n",
        "\n",
        "\n",
        "    # 5. Create or clear the 'Table of Contents' worksheet\n",
        "    try:\n",
        "        toc_sheet = spreadsheet.worksheet(\"Table of Contents\")\n",
        "        print(\"  -> Found existing 'Table of Contents' sheet. Clearing it.\")\n",
        "        toc_sheet.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(\"  -> 'Table of Contents' sheet not found. Creating a new one.\")\n",
        "        toc_sheet = spreadsheet.add_worksheet(title=\"Table of Contents\", rows=len(data_to_write) + 50, cols=4)\n",
        "\n",
        "    # Move ToC to the first position\n",
        "    spreadsheet.reorder_worksheets([toc_sheet] + [s for s in all_sheets if s.id != toc_sheet.id])\n",
        "    print(\"  -> Sheet positioned at the front.\")\n",
        "\n",
        "    # 6. Write the data and apply formatting\n",
        "    print(\"  -> Writing data and applying formatting...\")\n",
        "    toc_sheet.update(\n",
        "        'A1',\n",
        "        data_to_write,\n",
        "        value_input_option='USER_ENTERED' # IMPORTANT to correctly interpret formulas\n",
        "    )\n",
        "\n",
        "    # Add sheetId to all formatting requests\n",
        "    for req in formatting_requests:\n",
        "        if 'mergeCells' in req:\n",
        "            req['mergeCells']['range']['sheetId'] = toc_sheet.id\n",
        "        elif 'repeatCell' in req:\n",
        "            req['repeatCell']['range']['sheetId'] = toc_sheet.id\n",
        "\n",
        "    # Add column resize request\n",
        "    formatting_requests.append(\n",
        "        {\"autoResizeDimensions\": {\n",
        "            \"dimensions\": {\"sheetId\": toc_sheet.id, \"dimension\": \"COLUMNS\", \"startIndex\": 0, \"endIndex\": 4}\n",
        "        }}\n",
        "    )\n",
        "\n",
        "    if formatting_requests:\n",
        "        spreadsheet.batch_update({'requests': formatting_requests})\n",
        "\n",
        "    print(\"✅ --- Table of Contents generation complete! ---\")\n",
        "\n",
        "\n",
        "# --- Call the new function ---\n",
        "create_table_of_contents(SPREADSHEET_URL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8nnkG4tdEvM+4cmmziYcg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}