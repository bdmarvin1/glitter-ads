{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBuDLPbjbk1RZiGWQJv8zG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdmarvin1/glitter-ads/blob/main/Google_Ads_Trend_Reporting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xwZlUMCKxo9"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 1: INSTALL REQUIRED LIBRARIES\n",
        "# ==============================================================================\n",
        "!pip install google-ads google-auth-oauthlib gspread python-dateutil google-api-python-client\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: IMPORTS & AUTHENTICATION\n",
        "# ==============================================================================\n",
        "import gspread\n",
        "from gspread.utils import rowcol_to_a1\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "from google.ads.googleads.client import GoogleAdsClient\n",
        "from google.ads.googleads.errors import GoogleAdsException\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# --- Authenticate and Initialize Clients ---\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/documents']\n",
        "    creds, _ = default(scopes=SCOPES)\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"✅ Google Sheets & Drive clients initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An unexpected error occurred during Google auth: {e}\")\n",
        "\n",
        "google_ads_client = None\n",
        "try:\n",
        "    print(\"--- Initializing Google Ads Client ---\")\n",
        "    credentials = {\n",
        "        \"developer_token\": userdata.get('ADS_DEVELOPER_TOKEN'),\n",
        "        \"client_id\": userdata.get('ADS_CLIENT_ID'),\n",
        "        \"client_secret\": userdata.get('ADS_CLIENT_SECRET'),\n",
        "        \"refresh_token\": userdata.get('ADS_REFRESH_TOKEN'),\n",
        "        \"login_customer_id\": userdata.get('ADS_LOGIN_CUSTOMER_ID'),\n",
        "        \"use_proto_plus\": True\n",
        "    }\n",
        "    google_ads_client = GoogleAdsClient.load_from_dict(credentials)\n",
        "    print(\"✅ Google Ads client initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An unexpected error occurred during Ads client setup: {e}\")\n",
        "\n",
        "docs_service = None\n",
        "try:\n",
        "    print(\"--- Initializing Google Docs Client ---\")\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "    print(\"✅ Google Docs client initialized successfully!\")\n",
        "except HttpError as e:\n",
        "    print(f\"\\n❌ An error occurred during Google Docs client setup: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An unexpected error occurred during Docs client setup: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: CONFIGURATION\n",
        "# ==============================================================================\n",
        "DOCS_LOG_ID = '1ZMv52EXlCDNX0sWBDcQRNdvuOxRcdOogMy0jHGvVgTE'\n",
        "SPREADSHEET_URL = 'https://docs.google.com/spreadsheets/d/13QJZGvR8Zp_floekTod4V3LvLZKLfKdCLFjCKRhU2tQ/edit?gid=0#gid=0'\n",
        "KEYWORD_GROUPS = {\n",
        "    'General Storage': [\"self storage\", \"self storage unit\", \"self storage units\", \"storage\", \"storage facilities\", \"storage facility\", \"storage for rent\", \"storage locker\", \"storage unit\", \"storage units\"],\n",
        "    'Near Me': [\"storage units near me\", \"storage unit near me\", \"storage facility near me\", \"self storage near me\", \"business storage near me\", \"vehicle storage near me\", \"boat storage near me\", \"rv storage near me\", \"car storage near me\", \"climate controlled storage near me\"],\n",
        "    'Features': [\"drive up storage units\", \"drive up storage\", \"drive up storage facility\", \"climate controlled storage\", \"climate controlled storage unit\", \"climate controlled storage units\", \"heated storage\", \"heated storage units\", \"temperature control storage\", \"temperature controlled storage\", \"covered boat storage\", \"covered rv storage\", \"mini storage\", \"indoor storage units\", \"indoor storage\", \"indoor self storage\", \"outdoor storage\", \"outdoor storage units\", \"24 hour storage\"],\n",
        "    'Customer Types': [\"business storage\", \"construction storage\", \"contractor storage\", \"commercial storage\", \"commercial storage units\", \"military storage\", \"student storage\", \"wine storage\", \"affordable storage\", \"cheap storage units\"],\n",
        "    'Vehicle Storage': [\"auto storage\", \"boat parking\", \"boat storage\", \"car storage\", \"camper storage\", \"motorcycle storage\", \"rv storage\", \"truck storage\", \"trailer parking\", \"trailer storage\", \"vehicle storage\", \"5x5 storage unit\", \"10x10 storage unit\", \"10x15 storage unit\", \"10x20 storage unit\"]\n",
        "}\n",
        "US_STATES = [\n",
        "  \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "US_CITIES = [\n",
        "  \"New York, NY\", \"Los Angeles, CA\", \"Chicago, IL\", \"Houston, TX\", \"Phoenix, AZ\", \"Philadelphia, PA\", \"San Antonio, TX\", \"San Diego, CA\", \"Dallas, TX\", \"Jacksonville, FL\", \"Fort Worth, TX\", \"San Jose, CA\", \"Austin, TX\", \"Charlotte, NC\", \"Columbus, OH\", \"Indianapolis, IN\", \"San Francisco, CA\", \"Seattle, WA\", \"Denver, CO\", \"Oklahoma City, OK\", \"Nashville, TN\", \"Washington, DC\", \"El Paso, TX\", \"Las Vegas, NV\", \"Boston, MA\", \"Detroit, MI\", \"Louisville, KY\", \"Portland, OR\", \"Memphis, TN\", \"Baltimore, MD\", \"Milwaukee, WI\", \"Albuquerque, NM\", \"Tucson, AZ\", \"Fresno, CA\", \"Sacramento, CA\", \"Atlanta, GA\", \"Mesa, AZ\", \"Kansas City, MO\", \"Raleigh, NC\", \"Colorado Springs, CO\", \"Omaha, NE\", \"Miami, FL\", \"Virginia Beach, VA\", \"Long Beach, CA\", \"Oakland, CA\", \"Minneapolis, MN\", \"Bakersfield, CA\", \"Tulsa, OK\", \"Tampa, FL\", \"Arlington, TX\", \"Aurora, CO\", \"Wichita, KS\", \"Cleveland, OH\", \"New Orleans, LA\", \"Henderson, NV\", \"Honolulu, HI\", \"Anaheim, CA\", \"Orlando, FL\", \"Lexington, KY\", \"Stockton, CA\", \"Riverside, CA\", \"Irvine, CA\", \"Corpus Christi, TX\", \"Newark, NJ\", \"Santa Ana, CA\", \"Cincinnati, OH\", \"Pittsburgh, PA\", \"Saint Paul, MN\", \"Greensboro, NC\", \"Jersey City, NJ\", \"Durham, NC\", \"Lincoln, NE\", \"North Las Vegas, NV\", \"Plano, TX\", \"Anchorage, AK\", \"Gilbert, AZ\", \"Madison, WI\", \"Reno, NV\", \"Chandler, AZ\", \"St. Louis, MO\", \"Chula Vista, CA\", \"Buffalo, NY\", \"Fort Wayne, IN\", \"Lubbock, TX\", \"St. Petersburg, FL\", \"Toledo, OH\", \"Laredo, TX\", \"Port St. Lucie, FL\", \"Glendale, AZ\", \"Irving, TX\", \"Winston-Salem, NC\", \"Chesapeake, VA\", \"Garland, TX\", \"Scottsdale, AZ\", \"Boise, ID\", \"Hialeah, FL\", \"Frisco, TX\", \"Richmond, VA\", \"Cape Coral, FL\", \"Norfolk, VA\", \"Spokane, WA\", \"Huntsville, AL\", \"Santa Clarita, CA\", \"Tacoma, WA\", \"Fremont, CA\", \"McKinney, TX\", \"San Bernardino, CA\", \"Baton Rouge, LA\", \"Modesto, CA\", \"Fontana, CA\", \"Salt Lake City, UT\", \"Moreno Valley, CA\", \"Des Moines, IA\", \"Worcester, MA\", \"Yonkers, NY\", \"Fayetteville, NC\", \"Sioux Falls, SD\", \"Grand Prairie, TX\", \"Rochester, NY\", \"Tallahassee, FL\", \"Little Rock, AR\", \"Amarillo, TX\", \"Overland Park, KS\", \"Columbus, GA\", \"Augusta, GA\", \"Mobile, AL\", \"Oxnard, CA\", \"Grand Rapids, MI\", \"Peoria, AZ\", \"Vancouver, WA\", \"Knoxville, TN\", \"Birmingham, AL\", \"Montgomery, AL\", \"Providence, RI\", \"Huntington Beach, CA\", \"Brownsville, TX\", \"Chattanooga, TN\", \"Fort Lauderdale, FL\", \"Tempe, AZ\", \"Akron, OH\", \"Glendale, CA\", \"Clarksville, TN\", \"Ontario, CA\", \"Newport News, VA\", \"Elk Grove, CA\", \"Cary, NC\", \"Aurora, IL\", \"Salem, OR\", \"Pembroke Pines, FL\", \"Eugene, OR\", \"Santa Rosa, CA\", \"Rancho Cucamonga, CA\", \"Shreveport, LA\", \"Garden Grove, CA\", \"Oceanside, CA\", \"Fort Collins, CO\", \"Springfield, MO\", \"Murfreesboro, TN\", \"Surprise, AZ\", \"Lancaster, CA\", \"Denton, TX\", \"Roseville, CA\", \"Palmdale, CA\", \"Corona, CA\", \"Salinas, CA\", \"Killeen, TX\", \"Paterson, NJ\", \"Alexandria, VA\", \"Hollywood, FL\", \"Hayward, CA\", \"Charleston, SC\", \"Macon, GA\", \"Lakewood, CO\", \"Sunnyvale, CA\", \"Kansas City, KS\", \"Springfield, MA\", \"Bellevue, WA\", \"Naperville, IL\", \"Joliet, IL\", \"Bridgeport, CT\", \"Mesquite, TX\", \"Pasadena, TX\", \"Olathe, KS\", \"Escondido, CA\", \"Savannah, GA\", \"McAllen, TX\", \"Gainesville, FL\", \"Pomona, CA\", \"Rockford, IL\", \"Thornton, CO\", \"Waco, TX\", \"Visalia, CA\", \"Syracuse, NY\", \"Columbia, SC\", \"Midland, TX\", \"Miramar, FL\", \"Palm Bay, FL\", \"Lakewood, NJ\", \"Jackson, MS\", \"Coral Springs, FL\", \"Victorville, CA\", \"Elizabeth, NJ\", \"Fullerton, CA\", \"Meridian, ID\", \"Torrance, CA\", \"Stamford, CT\", \"West Valley City, UT\", \"Orange, CA\", \"Cedar Rapids, IA\", \"Warren, MI\", \"Hampton, VA\", \"New Haven, CT\", \"Pasadena, CA\", \"Kent, WA\", \"Dayton, OH\", \"Fargo, ND\", \"Lewisville, TX\", \"Carrollton, TX\", \"Round Rock, TX\", \"Sterling Heights, MI\", \"Santa Clara, CA\", \"Norman, OK\", \"Columbia, MO\", \"Abilene, TX\", \"Pearland, TX\", \"Athens, GA\", \"College Station, TX\", \"Clovis, CA\", \"West Palm Beach, FL\", \"Allentown, PA\", \"North Charleston, SC\", \"Simi Valley, CA\", \"Topeka, KS\", \"Wilmington, NC\", \"Lakeland, FL\", \"Thousand Oaks, CA\", \"Concord, CA\", \"Rochester, MN\", \"Vallejo, CA\", \"Ann Arbor, MI\", \"Broken Arrow, OK\", \"Fairfield, CA\", \"Lafayette, LA\", \"Hartford, CT\", \"Arvada, CO\", \"Berkeley, CA\", \"Independence, MO\", \"Billings, MT\", \"Cambridge, MA\", \"Lowell, MA\", \"Odessa, TX\", \"High Point, NC\", \"League City, TX\", \"Antioch, CA\", \"Richardson, TX\", \"Goodyear, AZ\", \"Pompano Beach, FL\", \"Nampa, ID\", \"Menifee, CA\", \"Las Cruces, NM\", \"Clearwater, FL\", \"West Jordan, UT\", \"New Braunfels, TX\", \"Manchester, NH\", \"Miami Gardens, FL\", \"Waterbury, CT\", \"Provo, UT\", \"Evansville, IN\", \"Richmond, CA\", \"Westminster, CO\", \"Elgin, IL\", \"Conroe, TX\", \"Greeley, CO\", \"Lansing, MI\", \"Buckeye, AZ\", \"Tuscaloosa, AL\", \"Allen, TX\", \"Carlsbad, CA\", \"Everett, WA\", \"Springfield, IL\", \"Beaumont, TX\", \"Murrieta, CA\", \"Rio Rancho, NM\", \"Temecula, CA\", \"Concord, NC\", \"Tyler, TX\", \"Davie, FL\", \"South Fulton, GA\", \"Peoria, IL\", \"Sparks, NV\", \"Gresham, OR\", \"Santa Maria, CA\", \"Pueblo, CO\", \"Hillsboro, OR\", \"Edison, NJ\", \"Sugar Land, TX\", \"Ventura, CA\", \"Downey, CA\", \"Costa Mesa, CA\", \"Centennial, CO\", \"Edinburg, TX\", \"Spokane Valley, WA\", \"Jurupa Valley, CA\", \"Bend, OR\", \"West Covina, CA\", \"Boulder, CO\", \"Palm Coast, FL\", \"Lee's Summit, MO\", \"Dearborn, MI\", \"Green Bay, WI\", \"St. George, UT\", \"Woodbridge, NJ\", \"Brockton, MA\", \"Renton, WA\", \"Sandy Springs, GA\", \"Rialto, CA\", \"El Monte, CA\", \"Vacaville, CA\", \"Fishers, IN\", \"South Bend, IN\", \"Carmel, IN\", \"Yuma, AZ\", \"Burbank, CA\", \"Lynn, MA\", \"Quincy, MA\", \"El Cajon, CA\", \"Fayetteville, AR\", \"Suffolk, VA\", \"San Mateo, CA\", \"Chico, CA\", \"Inglewood, CA\", \"Wichita Falls, TX\", \"Boca Raton, FL\", \"Hesperia, CA\", \"Daly City, CA\", \"Clinton, MI\", \"Georgetown, TX\", \"New Bedford, MA\", \"Albany, NY\", \"Davenport, IA\", \"Plantation, FL\", \"Deltona, FL\", \"Federal Way, WA\", \"San Angelo, TX\", \"Tracy, CA\", \"Sunrise, FL\"\n",
        "]\n",
        "CA_PROVINCES = [\"Alberta\", \"British Columbia\", \"Ontario\", \"Quebec\", \"Manitoba\", \"New Brunswick\", \"Newfoundland and Labrador\", \"Nova Scotia\", \"Prince Edward Island\", \"Saskatchewan\"]\n",
        "CA_LOCATIONS = [ \"Winnipeg, Manitoba\", \"Dieppe, New Brunswick\", \"Fredericton, New Brunswick\", \"Greater Lakeburn, New Brunswick\", \"Moncton, New Brunswick\", \"Saint John, New Brunswick\", \"Dartmouth, Nova Scotia\", \"Halifax, Nova Scotia\", \"Aurora, Ontario\", \"Barrie, Ontario\", \"Bowmanville, Ontario\", \"Chatham, Ontario\", \"Collingwood, Ontario\", \"East Gwillimbury, Ontario\", \"Goderich, Ontario\", \"Holland Landing, Ontario\", \"Leamington, Ontario\", \"London, Ontario\", \"Midland, Ontario\", \"Mississauga, Ontario\", \"Newmarket, Ontario\", \"Niagara Falls, Ontario\", \"North York, Ontario\", \"Oakville, Ontario\", \"Ottawa, Ontario\", \"Peterborough, Ontario\", \"Port Carling, Ontario\", \"Queensville, Ontario\", \"Richmond Hill, Ontario\", \"Seguin, Ontario\", \"St. Catharines, Ontario\", \"St. Marys, Ontario\", \"Sudbury, Ontario\", \"Thunder Bay, Ontario\", \"Toronto, Ontario\", \"Waterloo, Ontario\", \"Welland, Ontario\", \"Guelph, Ontario\", \"Scarborough, Ontario\"]\n",
        "LOCATIONS_TO_CHECK = (\n",
        "    [\"United States\"]\n",
        "    + US_STATES\n",
        "    + US_CITIES\n",
        "    + [\"Canada\"]\n",
        "    + CA_PROVINCES\n",
        "    + CA_LOCATIONS\n",
        ")"
      ],
      "metadata": {
        "id": "e4SzSYachdXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: CORE LOGIC & HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def get_location_criterion_id(client, location_name):\n",
        "    \"\"\"Converts a location name into a Geo Target Constant Criterion ID.\"\"\"\n",
        "    gtc_service = client.get_service(\"GeoTargetConstantService\")\n",
        "    request = client.get_type(\"SuggestGeoTargetConstantsRequest\")\n",
        "    request.location_names.names.append(location_name)\n",
        "    try:\n",
        "        response = gtc_service.suggest_geo_target_constants(request)\n",
        "        if not response.geo_target_constant_suggestions:\n",
        "            print(f\"    -> ⚠️  Could not find a location criterion for '{location_name}'.\")\n",
        "            return None\n",
        "        return response.geo_target_constant_suggestions[0].geo_target_constant.resource_name\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f\"    -> ❌ Failed to get location ID for '{location_name}': {ex}\")\n",
        "        return None\n",
        "\n",
        "def get_historical_metrics(client, customer_id, keywords, location_criterion):\n",
        "    \"\"\"\n",
        "    Queries the Google Ads API for historical search volume data, starting from a fixed date (Jan 2023)\n",
        "    up to the last full month.\n",
        "    \"\"\"\n",
        "    print(f\"  -> Fetching data for location: '{location_criterion}'...\")\n",
        "\n",
        "    request = client.get_type(\"GenerateKeywordHistoricalMetricsRequest\")\n",
        "    request.customer_id = customer_id\n",
        "    request.keywords = keywords\n",
        "    request.geo_target_constants.append(location_criterion)\n",
        "\n",
        "    # --- UPDATED: Date Range Calculation ---\n",
        "    # The end date is the last day of the previous month.\n",
        "    today = datetime.date.today()\n",
        "    last_full_month_date = today.replace(day=1) - relativedelta(days=1)\n",
        "    end_year = last_full_month_date.year\n",
        "    end_month_enum_value = getattr(client.get_type(\"MonthOfYearEnum\").MonthOfYear, last_full_month_date.strftime('%B').upper())\n",
        "\n",
        "    # The start date is now fixed to January 2023.\n",
        "    start_year = 2023\n",
        "    start_month_enum_value = client.get_type(\"MonthOfYearEnum\").MonthOfYear.JANUARY\n",
        "\n",
        "    print(f\"    -> Requesting historical data from {start_year}-01 to {end_year}-{last_full_month_date.month:02d}.\")\n",
        "\n",
        "    request.historical_metrics_options.year_month_range.start.year = start_year\n",
        "    request.historical_metrics_options.year_month_range.start.month = start_month_enum_value\n",
        "    request.historical_metrics_options.year_month_range.end.year = end_year\n",
        "    request.historical_metrics_options.year_month_range.end.month = end_month_enum_value\n",
        "\n",
        "    try:\n",
        "        time.sleep(1)\n",
        "        keyword_plan_idea_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "        response = keyword_plan_idea_service.generate_keyword_historical_metrics(request=request)\n",
        "\n",
        "        keyword_data = {}\n",
        "        for result in response.results:\n",
        "            monthly_volumes = {}\n",
        "            for s in result.keyword_metrics.monthly_search_volumes:\n",
        "                # The s.month enum from the API is 1-indexed (e.g., JANUARY = 1).\n",
        "                # The original code incorrectly subtracted 1, which shifted all months.\n",
        "                # This is the corrected, direct assignment.\n",
        "                year = s.year\n",
        "                month = s.month.value\n",
        "                month_key = f\"{year}-{month:02d}\"\n",
        "                monthly_volumes[month_key] = s.monthly_searches\n",
        "            keyword_data[result.text] = monthly_volumes\n",
        "\n",
        "        print(f\"    -> Successfully fetched data for {len(keyword_data)} keywords.\")\n",
        "        return keyword_data\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f'    -> ❌ Request failed: {ex.error.code().name}')\n",
        "        for error in ex.failure.errors:\n",
        "            print(f'       Error: {error.message} (Field: {error.location.field_path_elements[0].field_name})')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"    -> ❌ An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def aggregate_data(keyword_data, groups):\n",
        "    \"\"\"\n",
        "    Aggregates search volume by the predefined groups (summing).\n",
        "    This function now uses ALL unique months present in the API response data,\n",
        "    with no upper limit on the number of months.\n",
        "    \"\"\"\n",
        "    print(\"  -> Aggregating data...\")\n",
        "\n",
        "    # 1. Collect ALL unique YYYY-MM strings that actually exist in the API response.\n",
        "    all_unique_months_from_api = set()\n",
        "    for keyword_metrics in keyword_data.values():\n",
        "        for month_key in keyword_metrics.keys():\n",
        "            all_unique_months_from_api.add(month_key)\n",
        "\n",
        "    # 2. Sort these months chronologically. This is crucial for correct sheet ordering.\n",
        "    sorted_api_months = sorted(list(all_unique_months_from_api), key=lambda x: datetime.datetime.strptime(x, '%Y-%m'))\n",
        "\n",
        "    # 3. Use all sorted months. The 25-month limit has been removed.\n",
        "    final_months = sorted_api_months\n",
        "\n",
        "    print(f\"    -> Aggregating across {len(final_months)} total months.\")\n",
        "\n",
        "    # Initialize aggregated_volumes with the definitive month list (oldest to newest)\n",
        "    aggregated_volumes = {group: {month: 0 for month in final_months} for group in groups}\n",
        "\n",
        "    for group_name, keywords_in_group in groups.items():\n",
        "        for keyword in keywords_in_group:\n",
        "            if keyword in keyword_data:\n",
        "                for month, volume in keyword_data[keyword].items():\n",
        "                    # Check if the month is in our list (it always should be)\n",
        "                    if month in final_months:\n",
        "                        aggregated_volumes[group_name][month] += volume\n",
        "\n",
        "    print(\"    -> Aggregation complete.\")\n",
        "    return aggregated_volumes, final_months\n",
        "\n",
        "def update_google_sheet(spreadsheet_url, location_name, aggregated_data, months):\n",
        "    \"\"\"\n",
        "    Updates the sheet by duplicating a template, clearing A1:X#, and pasting new data.\n",
        "    Now dynamically adjusts for up to 25 months.\n",
        "    \"\"\"\n",
        "    print(f\"  -> Preparing to update sheet for '{location_name}'...\")\n",
        "    max_retries = 5\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "            try:\n",
        "                template_sheet = spreadsheet.worksheet(\"Sheet1\")\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                print(\"    -> ❌ CRITICAL ERROR: The template 'Sheet1' was not found.\"); return\n",
        "\n",
        "            try:\n",
        "                worksheet = spreadsheet.worksheet(location_name)\n",
        "                print(f\"    -> Found existing worksheet '{location_name}'.\")\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                print(f\"    -> Worksheet '{location_name}' not found. Duplicating from 'Sheet1' at end of list...\")\n",
        "\n",
        "                source_sheet_id = template_sheet.id\n",
        "                insert_index = len(spreadsheet.worksheets())\n",
        "\n",
        "                requests = [{\"duplicateSheet\": {\"sourceSheetId\": source_sheet_id, \"newSheetName\": location_name, \"insertSheetIndex\": insert_index}}]\n",
        "                spreadsheet.batch_update({\"requests\": requests})\n",
        "\n",
        "                worksheet = spreadsheet.worksheet(location_name)\n",
        "                print(f\"    -> New worksheet '{location_name}' created and positioned at the end.\")\n",
        "\n",
        "            group_names = list(KEYWORD_GROUPS.keys())\n",
        "            header_row = ['Keyword Group'] + months\n",
        "            volume_rows = [[group] + [aggregated_data[group].get(month, 0) for month in months] for group in group_names]\n",
        "\n",
        "            data_to_paste = [header_row] + volume_rows\n",
        "\n",
        "            num_rows_to_clear = len(data_to_paste)\n",
        "            num_cols_to_clear = len(data_to_paste[0])\n",
        "            clear_end_cell = rowcol_to_a1(num_rows_to_clear, num_cols_to_clear)\n",
        "            clear_range = f'A1:{clear_end_cell}'\n",
        "\n",
        "            print(f\"    -> Clearing target range: {clear_range}...\")\n",
        "            worksheet.batch_clear([clear_range])\n",
        "            print(\"    -> Pasting monthly search volume data into A1.\")\n",
        "            worksheet.update(range_name='A1', values=data_to_paste, value_input_option='USER_ENTERED')\n",
        "\n",
        "            worksheet.spreadsheet.batch_update({\n",
        "                \"requests\": [{\"autoResizeDimensions\": {\"dimensions\": {\"sheetId\": worksheet.id, \"dimension\": \"COLUMNS\", \"startIndex\": 0, \"endIndex\": num_cols_to_clear}}}]\n",
        "            })\n",
        "\n",
        "            print(f\"    -> ✅ Sheet '{location_name}' updated successfully!\")\n",
        "            return\n",
        "\n",
        "        except gspread.exceptions.APIError as e:\n",
        "            if e.response.status_code in [429, 500]:\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"    -> ⚠️ APIError (Status {e.response.status_code}). Retrying in {wait_time} seconds (attempt {attempt + 1}/{max_retries})...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"    -> ❌ An unrecoverable API error occurred while updating the Google Sheet: {e}\")\n",
        "                return\n",
        "        except Exception as e:\n",
        "            print(f\"    -> ❌ An unexpected error occurred while updating the Google Sheet: {e}\")\n",
        "            return\n",
        "\n",
        "    print(f\"    -> ❌ Failed to update sheet '{location_name}' after {max_retries} attempts.\")\n",
        "\n",
        "def delete_all_except_specified_sheets(spreadsheet_url, sheets_to_keep):\n",
        "    \"\"\"\n",
        "    Deletes all worksheets in a Google Sheet except those specified in sheets_to_keep.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Cleaning up Spreadsheet: Deleting unnecessary sheets ---\")\n",
        "    time.sleep(1)\n",
        "    try:\n",
        "        spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "        all_worksheets = spreadsheet.worksheets()\n",
        "\n",
        "        sheets_deleted = []\n",
        "        for worksheet in all_worksheets:\n",
        "            if worksheet.title not in sheets_to_keep:\n",
        "                print(f\"  -> Deleting sheet: '{worksheet.title}'...\")\n",
        "                spreadsheet.del_worksheet(worksheet)\n",
        "                sheets_deleted.append(worksheet.title)\n",
        "            else:\n",
        "                print(f\"  -> Keeping sheet: '{worksheet.title}'\")\n",
        "\n",
        "        if sheets_deleted:\n",
        "            print(f\"✅ Successfully deleted: {', '.join(sheets_deleted)}\")\n",
        "        else:\n",
        "            print(\"  -> No sheets to delete (all specified sheets were kept).\")\n",
        "\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        print(f\"❌ Error: Spreadsheet not found at URL: {spreadsheet_url}. Cannot clean up.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred during sheet cleanup: {e}\")\n",
        "\n",
        "def get_doc_content_length(service, document_id):\n",
        "    \"\"\"Gets the insertion point at the very end of the Google Doc.\"\"\"\n",
        "    try:\n",
        "        doc = service.documents().get(documentId=document_id, fields='body(content(endIndex))').execute()\n",
        "        # The endIndex of the last content element is the correct insertion point for appending.\n",
        "        return doc['body']['content'][-1]['endIndex']\n",
        "    except (HttpError, KeyError, IndexError) as e:\n",
        "        print(f\"    -> ⚠️  Could not determine document length (it might be empty): {e}. Falling back to index 1.\")\n",
        "        # 1 is the beginning of the document body, a safe place to insert.\n",
        "        return 1\n",
        "\n",
        "def batch_update_doc(service, document_id, requests):\n",
        "    \"\"\"Applies a batch of update requests to the Google Doc.\"\"\"\n",
        "    if not requests:\n",
        "        print(\"  -> No log data to send to Google Docs.\")\n",
        "        return\n",
        "    try:\n",
        "        service.documents().batchUpdate(\n",
        "            documentId=document_id,\n",
        "            body={'requests': requests}\n",
        "        ).execute()\n",
        "        print(\"  -> ✅ Successfully wrote logs to Google Doc.\")\n",
        "    except HttpError as e:\n",
        "        print(f\"    -> ❌ Error writing to Google Doc: {e}\")\n",
        "\n",
        "def create_state_summary_sheet(spreadsheet_url):\n",
        "    \"\"\"\n",
        "    Creates/updates a 'StateSummary' sheet with states as rows and months as columns.\n",
        "    It now intelligently appends summary columns for 1, 3, 6, 12, and 24-month\n",
        "    change and percentage change, based on the available data.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Creating/Updating State Summary Sheet (with Change Analysis) ---\")\n",
        "    try:\n",
        "        spreadsheet = gc.open_by_url(spreadsheet_url)\n",
        "\n",
        "        try:\n",
        "            summary_worksheet = spreadsheet.worksheet(\"StateSummary\")\n",
        "            print(\"  -> Found existing 'StateSummary' worksheet.\")\n",
        "        except gspread.exceptions.WorksheetNotFound:\n",
        "            print(\"  -> 'StateSummary' worksheet not found. Creating it...\")\n",
        "            summary_worksheet = spreadsheet.add_worksheet(title=\"StateSummary\", rows=\"100\", cols=\"70\", index=1)\n",
        "            print(\"  -> New 'StateSummary' worksheet created.\")\n",
        "\n",
        "        months = []\n",
        "        try:\n",
        "            sample_sheet = spreadsheet.worksheet(\"United States\")\n",
        "            months = sample_sheet.row_values(1)[1:]\n",
        "            if not months: raise ValueError(\"'United States' sheet is empty.\")\n",
        "            print(f\"  -> Successfully loaded {len(months)} month headers from 'United States' sheet.\")\n",
        "        except (gspread.exceptions.WorksheetNotFound, ValueError, IndexError) as e:\n",
        "            print(f\"  -> WARNING: Could not get month headers ({e}). Generating fallback.\")\n",
        "            start_date = datetime.date(2023, 1, 1)\n",
        "            end_date = datetime.date.today().replace(day=1) - relativedelta(days=1)\n",
        "            current_date = start_date\n",
        "            while current_date <= end_date:\n",
        "                months.append(current_date.strftime('%Y-%m'))\n",
        "                current_date += relativedelta(months=1)\n",
        "            print(f\"  -> Generated {len(months)} fallback month headers.\")\n",
        "\n",
        "        # --- Base data setup ---\n",
        "        header_row = ['State'] + months\n",
        "        data_rows = []\n",
        "        all_sheet_titles = [s.title for s in spreadsheet.worksheets()]\n",
        "        existing_state_sheets = [s for s in US_STATES if s in all_sheet_titles]\n",
        "\n",
        "        if not existing_state_sheets:\n",
        "            print(\"  -> No state-specific sheets found. Aborting summary creation.\")\n",
        "            return\n",
        "\n",
        "        for state_name in existing_state_sheets:\n",
        "            formula_row = [state_name]\n",
        "            for j in range(len(months)):\n",
        "                col_index = j + 2\n",
        "                a1_notation = rowcol_to_a1(1, col_index)\n",
        "                source_col_letter = ''.join(filter(str.isalpha, a1_notation))\n",
        "                formula = f\"='{state_name}'!{source_col_letter}7\"\n",
        "                formula_row.append(formula)\n",
        "            data_rows.append(formula_row)\n",
        "\n",
        "        # --- NEW: Add summary columns ---\n",
        "        summary_periods = [('1m', 1), ('3m', 3), ('6m', 6), ('12m', 12), ('24m', 24)]\n",
        "        num_months = len(months)\n",
        "        last_month_col_index = 1 + num_months\n",
        "        last_month_col_letter = ''.join(filter(str.isalpha, rowcol_to_a1(1, last_month_col_index)))\n",
        "\n",
        "        percentage_format_requests = []\n",
        "\n",
        "        for label, period_months in summary_periods:\n",
        "            # Only add summary columns if we have enough historical data\n",
        "            if num_months > period_months:\n",
        "                print(f\"  -> Adding {label} summary columns.\")\n",
        "                # Define new headers\n",
        "                change_header = f\"{label} Change\"\n",
        "                pct_change_header = f\"{label} % Change\"\n",
        "                header_row.extend([change_header, pct_change_header])\n",
        "\n",
        "                # Get the column for the base month (e.g., 12 months ago)\n",
        "                base_month_col_index = last_month_col_index - period_months\n",
        "                base_month_col_letter = ''.join(filter(str.isalpha, rowcol_to_a1(1, base_month_col_index)))\n",
        "\n",
        "                # Get the column for the \"Change\" value we are about to add\n",
        "                change_col_index = len(header_row) - 1\n",
        "                change_col_letter = ''.join(filter(str.isalpha, rowcol_to_a1(1, change_col_index)))\n",
        "\n",
        "                # Add the formulas to each data row\n",
        "                for i, row in enumerate(data_rows):\n",
        "                    sheet_row_num = i + 2 # +2 because sheet is 1-indexed and has a header\n",
        "\n",
        "                    # Formula for absolute change: =(LastMonth - BaseMonth)\n",
        "                    change_formula = f\"={last_month_col_letter}{sheet_row_num}-{base_month_col_letter}{sheet_row_num}\"\n",
        "\n",
        "                    # Formula for % change: =IFERROR(Change / BaseMonth, 0)\n",
        "                    pct_change_formula = f\"=IFERROR({change_col_letter}{sheet_row_num}/{base_month_col_letter}{sheet_row_num}, 0)\"\n",
        "\n",
        "                    row.extend([change_formula, pct_change_formula])\n",
        "\n",
        "                # Prepare a request to format the % change column\n",
        "                pct_column_index = len(header_row) - 1 # 0-indexed column\n",
        "                percentage_format_requests.append({\n",
        "                    \"repeatCell\": {\n",
        "                        \"range\": {\"sheetId\": summary_worksheet.id, \"startColumnIndex\": pct_column_index, \"endColumnIndex\": pct_column_index + 1},\n",
        "                        \"cell\": {\"userEnteredFormat\": {\"numberFormat\": {\"type\": \"PERCENT\", \"pattern\": \"0.00%\"}}},\n",
        "                        \"fields\": \"userEnteredFormat.numberFormat\"\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # --- Combine all rows for final pasting ---\n",
        "        final_rows_to_paste = [header_row] + data_rows\n",
        "\n",
        "        # --- Update the sheet ---\n",
        "        print(f\"  -> Preparing to update sheet with {len(final_rows_to_paste)} rows and {len(header_row)} columns.\")\n",
        "        summary_worksheet.clear()\n",
        "        summary_worksheet.update(range_name='A1', values=final_rows_to_paste, value_input_option='USER_ENTERED')\n",
        "\n",
        "        # --- Apply all formatting requests and resize ---\n",
        "        update_requests = [{\n",
        "            \"autoResizeDimensions\": {\"dimensions\": {\"sheetId\": summary_worksheet.id, \"dimension\": \"COLUMNS\", \"startIndex\": 0, \"endIndex\": len(header_row)}}\n",
        "        }] + percentage_format_requests\n",
        "\n",
        "        summary_worksheet.spreadsheet.batch_update({\"requests\": update_requests})\n",
        "\n",
        "        print(\"✅ 'StateSummary' sheet updated successfully with change analysis!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred while creating/updating the State Summary Sheet: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    print(\"\\n--- Starting Main Execution ---\")\n",
        "    if not google_ads_client or not docs_service:\n",
        "        print(\"\\n❌ Halting execution: A required client (Google Ads or Docs) is not initialized.\")\n",
        "        return\n",
        "\n",
        "    all_keywords = [kw for group in KEYWORD_GROUPS.values() for kw in group]\n",
        "    if not all_keywords:\n",
        "        print(\"No keywords found. Please add keywords.\")\n",
        "        return\n",
        "\n",
        "    # --- Initialize Logging with H2 Header ---\n",
        "    run_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    try:\n",
        "        # Get the end of the document to append the new header.\n",
        "        insert_at_index = get_doc_content_length(docs_service, DOCS_LOG_ID)\n",
        "\n",
        "        # Prepend a newline for spacing if the document isn't empty.\n",
        "        timestamp_text = f\"{run_timestamp}\"\n",
        "        full_header_text = f\"\\n{timestamp_text}\\n\" if insert_at_index > 1 else f\"{timestamp_text}\\n\"\n",
        "\n",
        "        # Determine the start index for the style, which is after the initial newline.\n",
        "        style_start_index = insert_at_index + 1 if insert_at_index > 1 else insert_at_index\n",
        "\n",
        "        log_requests = [\n",
        "            {\n",
        "                'insertText': {\n",
        "                    'location': {'index': insert_at_index},\n",
        "                    'text': full_header_text\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                'updateParagraphStyle': {\n",
        "                    'range': {\n",
        "                        'startIndex': style_start_index,\n",
        "                        'endIndex': style_start_index + len(timestamp_text)\n",
        "                    },\n",
        "                    'paragraphStyle': {'namedStyleType': 'HEADING_2'},\n",
        "                    'fields': 'namedStyleType'\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "        batch_update_doc(docs_service, DOCS_LOG_ID, log_requests)\n",
        "    except Exception as e:\n",
        "        print(f\"  -> ❌ Could not write initial header to Google Doc: {e}\")\n",
        "\n",
        "\n",
        "    # This list will hold the log strings for the current batch.\n",
        "    log_batch = []\n",
        "\n",
        "    # Optional: Enable sheet cleanup\n",
        "    sheets_to_preserve = ['Sheet1', 'StateSummary']\n",
        "    delete_all_except_specified_sheets(SPREADSHEET_URL, sheets_to_preserve)\n",
        "\n",
        "    customer_id = credentials['login_customer_id'].replace(\"-\", \"\")\n",
        "\n",
        "    for i, location_name_str_raw in enumerate(LOCATIONS_TO_CHECK):\n",
        "        location_name_str = location_name_str_raw.strip()\n",
        "\n",
        "        # Determine the fully qualified location name for the API\n",
        "        if location_name_str in US_CITIES or location_name_str in CA_LOCATIONS:\n",
        "            # Already fully qualified, e.g., \"Los Angeles, CA\" or \"Toronto, Ontario\"\n",
        "            qualified_location_name = location_name_str\n",
        "        elif location_name_str in US_STATES:\n",
        "            # e.g., \"California\" -> \"California, United States\"\n",
        "            qualified_location_name = f\"{location_name_str}, United States\"\n",
        "        elif location_name_str in CA_PROVINCES:\n",
        "            # e.g., \"Ontario\" -> \"Ontario, Canada\"\n",
        "            qualified_location_name = f\"{location_name_str}, Canada\"\n",
        "        elif location_name_str == \"United States\" or location_name_str == \"Canada\":\n",
        "            # Whole country\n",
        "            qualified_location_name = location_name_str\n",
        "        else:\n",
        "            # Should not be reached, but as a fallback, use the name as is\n",
        "            qualified_location_name = location_name_str\n",
        "\n",
        "        print(f\"\\n--- Processing Location ({i+1}/{len(LOCATIONS_TO_CHECK)}): {location_name_str} (as '{qualified_location_name}') ---\")\n",
        "\n",
        "        log_entry = f\"Processing: {location_name_str}... \"\n",
        "\n",
        "        location_criterion_resource_name = get_location_criterion_id(google_ads_client, qualified_location_name)\n",
        "\n",
        "        if location_criterion_resource_name:\n",
        "            keyword_metrics = get_historical_metrics(google_ads_client, customer_id, all_keywords, location_criterion_resource_name)\n",
        "            if keyword_metrics:\n",
        "                aggregated_data, months = aggregate_data(keyword_metrics, KEYWORD_GROUPS)\n",
        "                update_google_sheet(SPREADSHEET_URL, location_name_str, aggregated_data, months)\n",
        "                log_entry += \"✅ Success\\n\"\n",
        "            else:\n",
        "                print(f\"  -> Skipping sheet update for '{location_name_str}' due to data fetch failure.\")\n",
        "                log_entry += \"❌ Failed (Data Fetch Error)\\n\"\n",
        "        else:\n",
        "            print(f\"  -> Skipping '{location_name_str}' due to invalid location name.\")\n",
        "            log_entry += \"❌ Failed (Invalid Location)\\n\"\n",
        "\n",
        "        log_batch.append(log_entry)\n",
        "\n",
        "        if len(log_batch) >= 10:\n",
        "            print(\"\\n--- Writing batch of 10 logs to Google Doc ---\")\n",
        "            full_log_text = \"\".join(log_batch)\n",
        "            try:\n",
        "                insert_at_index = get_doc_content_length(docs_service, DOCS_LOG_ID)\n",
        "                requests = [{'insertText': {'location': {'index': insert_at_index}, 'text': full_log_text}}]\n",
        "                batch_update_doc(docs_service, DOCS_LOG_ID, requests)\n",
        "                log_batch.clear()\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ❌ Could not write log batch to Google Doc: {e}\")\n",
        "\n",
        "    if log_batch:\n",
        "        print(\"\\n--- Writing final batch of logs to Google Doc ---\")\n",
        "        full_log_text = \"\".join(log_batch)\n",
        "        try:\n",
        "            insert_at_index = get_doc_content_length(docs_service, DOCS_LOG_ID)\n",
        "            requests = [{'insertText': {'location': {'index': insert_at_index}, 'text': full_log_text}}]\n",
        "            batch_update_doc(docs_service, DOCS_LOG_ID, requests)\n",
        "        except Exception as e:\n",
        "            print(f\"  -> ❌ Could not write final log batch to Google Doc: {e}\")\n",
        "\n",
        "    print(\"\\n\\n--- All data processing complete. Creating summary sheet... ---\")\n",
        "    create_state_summary_sheet(SPREADSHEET_URL)\n",
        "    print(f\"\\n--- All operations complete. ---\")\n",
        "    print(f\"View the results here: {SPREADSHEET_URL}\")"
      ],
      "metadata": {
        "id": "Sz2r7mJthzrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 7: STANDALONE TEST FUNCTION (Save Raw API Response)\n",
        "# ==============================================================================\n",
        "\n",
        "def test_api_response_and_save(test_keyword: str, test_location_name: str, output_filename=\"raw_api_test_response.txt\"):\n",
        "    \"\"\"\n",
        "    Makes a single Google Ads API call for a specific keyword and location,\n",
        "    and saves the raw API response's string representation to a text file for debugging.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running Standalone API Test for Keyword: '{test_keyword}' in '{test_location_name}' ---\")\n",
        "\n",
        "    if not google_ads_client:\n",
        "        print(\"  ❌ Google Ads client is not initialized. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    customer_id_for_api = credentials['login_customer_id'].replace(\"-\", \"\")\n",
        "\n",
        "    # Get location criterion ID\n",
        "    location_criterion_resource_name = get_location_criterion_id(google_ads_client, test_location_name)\n",
        "    if not location_criterion_resource_name:\n",
        "        return\n",
        "\n",
        "    # --- Use the same date logic as the main function for consistency ---\n",
        "    today = datetime.date.today()\n",
        "    last_full_month_date = today.replace(day=1) - relativedelta(days=1)\n",
        "    end_year = last_full_month_date.year\n",
        "    end_month_enum_value = getattr(google_ads_client.get_type(\"MonthOfYearEnum\").MonthOfYear, last_full_month_date.strftime('%B').upper())\n",
        "    start_year = 2023\n",
        "    start_month_enum_value = google_ads_client.get_type(\"MonthOfYearEnum\").MonthOfYear.JANUARY\n",
        "    print(f\"  -> Requesting historical data from {start_year}-01 to {end_year}-{last_full_month_date.month:02d}.\")\n",
        "\n",
        "    api_request = google_ads_client.get_type(\"GenerateKeywordHistoricalMetricsRequest\")\n",
        "    api_request.customer_id = customer_id_for_api\n",
        "    api_request.keywords.append(test_keyword)\n",
        "    api_request.geo_target_constants.append(location_criterion_resource_name)\n",
        "    api_request.historical_metrics_options.year_month_range.start.year = start_year\n",
        "    api_request.historical_metrics_options.year_month_range.start.month = start_month_enum_value\n",
        "    api_request.historical_metrics_options.year_month_range.end.year = end_year\n",
        "    api_request.historical_metrics_options.year_month_range.end.month = end_month_enum_value\n",
        "\n",
        "    try:\n",
        "        keyword_plan_idea_service = google_ads_client.get_service(\"KeywordPlanIdeaService\")\n",
        "        print(f\"  -> Sending API request for keyword '{test_keyword}' in '{test_location_name}'...\")\n",
        "        raw_api_response = keyword_plan_idea_service.generate_keyword_historical_metrics(request=api_request)\n",
        "\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(str(raw_api_response))\n",
        "\n",
        "        print(f\"  ✅ Raw API response (string representation) successfully saved to '{output_filename}'.\")\n",
        "        print(\"     You can view this file in Colab's file browser (left sidebar -> folder icon).\")\n",
        "    except GoogleAdsException as ex:\n",
        "        print(f\"  ❌ API Request failed: {ex.error.code().name}.\")\n",
        "        for error in ex.failure.errors:\n",
        "            print(f\"    -> Error message: {error.message}\")\n",
        "            if error.location:\n",
        "                for field_path_element in error.location.field_path_elements:\n",
        "                    print(f\"      -> On field: {field_path_element.field_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 8: MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "uu6arkdQezBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}